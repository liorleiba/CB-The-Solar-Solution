{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1272071",
   "metadata": {},
   "source": [
    "# A broad light in a world full of petrol\n",
    "\n",
    "When residential solar energy equipment was first made available to homeowners it was costly. The people who made the switch were usually those who wanted to make a conscious effort to reduce their carbon footprints. But they also had the means to invest in the equipment and services needed to make that switch. In the beginning, like many other new technologies, initial adopters often paid more until the technologies become more mainstream.\n",
    "\n",
    "Yes, solar energy is increasingly becoming a viable fuel source for everyone. Today, switching to solar energy is far more affordable with the help of programs like the Investment Tax Credit (ITC). This incentive provides a tax credit of 26 percent of the cost to install solar power at your home. While this credit is in effect at 26% until the end of 2022, once it expires, solar energy will still remain a low-priced source of power as prices will most likely continue to drop. Meanwhile, oil, gas, and coal prices are likely to continue to increase, especially as sources are depleted and the costs to obtain these fuels grow.\n",
    "\n",
    "In the following report, we'll explore the alternate solution to the energy problem which is the solar market, we'll research it's growth and it's spread globally in the past years, to see how the world is running into the inevitable shifting to using solar energy as the planet's main resource.\n",
    "\n",
    "*Note: The data we will use contains the initials 'GEM', which stands for Global Energy Monitor, an organization that monitors the global energy status around the world. The data you'll see was taken straightly from the GEM data sources.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a7bdc4",
   "metadata": {},
   "source": [
    "<div style=\"border:solid green 2px; padding: 5px\"> <h1 style=\"color:green; margin-bottom:20px\">Note for the reader!</h1>\n",
    "<p style=\"font-size: 16px\"><strong>It is strongly and highly recommended to run the <u>entire</u> Jupyther Notebook from the beginning</strong>, as the missing values handling is operating mapping processes using coordinates, and the whole processes combined take averagely 75:16 minutes (1:15 hrs) to run.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b742a",
   "metadata": {},
   "source": [
    "---------------\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f0c40",
   "metadata": {},
   "source": [
    "# 1. Initialization\n",
    "\n",
    "We'll start by importing the necessary libraries for the research ahead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec1c33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import scipy.interpolate as intp\n",
    "import math as mt\n",
    "import random as rd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from time import perf_counter\n",
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "\n",
    "# Geo-Mapping libraries:\n",
    "try: # importing geopy\n",
    "    import geopy.geocoders as gc\n",
    "    from geopy.geocoders import Nominatim\n",
    "    from geopy.extra.rate_limiter import RateLimiter\n",
    "    from geopy.point import Point\n",
    "except: # installing geopy using pip (in case environment lack the library)\n",
    "    !pip install geopy\n",
    "    import geopy.geocoders as gc\n",
    "    from geopy.geocoders import Nominatim\n",
    "    from geopy.extra.rate_limiter import RateLimiter\n",
    "    from geopy.point import Point\n",
    "\n",
    "# Translations libraries & requirements for the libraries:\n",
    "try: # installing translators\n",
    "    import translators as ts\n",
    "except: # installing translators using pip (in case environment lack the library)\n",
    "    ! pip install translators --upgrade\n",
    "    import translators as ts\n",
    "try: # installing loguru\n",
    "    import loguru\n",
    "except: # installing loguru using pip (in case environment lack the library)\n",
    "    ! pip install loguru\n",
    "    import loguru\n",
    "try: # installing pathos\n",
    "    import pathos\n",
    "except: # installing pathos using pip (in case environment lack the library)\n",
    "    ! pip install pathos\n",
    "    import pathos\n",
    "\n",
    "# Data visualizations libraries:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "try: # importing missingno\n",
    "    import missingno as msno\n",
    "except: # installing missingno using pip (in case environment lack the library)\n",
    "    !pip install missingno\n",
    "    import missingno\n",
    "try: # importing plotly\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.figure_factory as ff\n",
    "except: # installing plotly using pip (in case environment lack the library)\n",
    "    !pip install plotly-express\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f6978",
   "metadata": {},
   "source": [
    "Creating an index slicing variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0336f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an Index Slice variable -\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283cab6",
   "metadata": {},
   "source": [
    "### 1.1 Load Data\n",
    "\n",
    "Now the raw data will be loaded into a DataFrame variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Loading the data we'll use for analysis -\n",
    "global_solar = pd.read_excel(\"C:\\DataBases\\CBridge\\Global-Solar-Power-Tracker-May-2022.xlsx\", \\\n",
    "                                 sheet_name='Data')\n",
    "\n",
    "# B) For the sake of simplicity, these following DFs \n",
    "# will represent the tables from the 'About' section in the original xlsx file of the DataBase:\n",
    "# 1. Columns data -\n",
    "cols_data = pd.read_excel(\"C:\\DataBases\\CBridge\\Global-Solar-Power-Tracker-May-2022.xlsx\", \\\n",
    "                          sheet_name='Columns')\n",
    "# 2. definition of each status value in the `status` column -\n",
    "status_def = pd.read_excel(\"C:\\DataBases\\CBridge\\Global-Solar-Power-Tracker-May-2022.xlsx\", \\\n",
    "                           sheet_name='Status_Definition')\n",
    "# 3. Solar farms capacity cut-off values in MWs -\n",
    "cutoff_thresholds = pd.read_excel(\"C:\\DataBases\\CBridge\\Global-Solar-Power-Tracker-May-2022.xlsx\", \\\n",
    "                           sheet_name='MW_Cutoff_Thresholds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bfb3b",
   "metadata": {},
   "source": [
    "Since data is expected to be pretty big and include a big amount of columns, I'll set the print options to print *all* columns in every time we'll display a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5717e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the DF printing to print all columns in each DF we'll print -\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692abf8",
   "metadata": {},
   "source": [
    "### 1.2 Introduction to the data\n",
    "\n",
    "At the current section, we'll study the data from the 'About' section of the xlsx file we imported the data from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9acc1",
   "metadata": {},
   "source": [
    "#### A) Columns coverage\n",
    "\n",
    "The raw data contains the following columns:\n",
    "- **`Country`** - Country the project is located in (Unabbreviated)\n",
    "\n",
    "\n",
    "- **`Project Name`** - Commonly used name.\n",
    "    - **Additional Note:** Preference is for a government provided name. Without that information, the name the owner or operating company uses was taken.\n",
    "\n",
    "\n",
    "- **`Phase name`** - Common name of the project phase.\n",
    "    - **Additional Note:** Roman numerals are changed to numbers. If the Owner uses non-sequential numbers or other characters, those are used.\n",
    "\n",
    "\n",
    "- **`Project Name in Local Language / Script`** - Commonly used name in the locally used script.\n",
    "    - **Additional Note:** Left blank for local languages that use the roman/latin alphabet. Otherwise local name in local script provided if possible\n",
    "\n",
    "\n",
    "- **`Other plant names`** - Alternative names, including the name in local script.\n",
    "\n",
    "\n",
    "- **`Capacity` (MW)** - Nameplate capacity, or best available data, of the phase in megawatts rounded to whole number.\n",
    "    - **Additional Note:** Solar farm capacity cut-off, Arab Countries: <10 MW not included; Rest of world: <20 MW not included\n",
    "\n",
    "\n",
    "- **`Capacity Rating` (MWac, MWp/dc, unknown)** - The reported capacity rating, unknown selected if not provided.\n",
    "    - **Additional Note:** Capacity may be reported as the DC / peak value the arrays could provide or as the AC value of what will be tied into the grid. This is often not specified and could be either value when reported as just 'MW'\n",
    "\n",
    "\n",
    "- **`Status`** - *see section below*.\n",
    "\n",
    "\n",
    "- **`Start year`** - Year the project is or is expected to be commissioned.\n",
    "    - **Additional Note:** Latest date was taken when original data source provided a range; Start years not included for projects that are canceled or shelved\n",
    "\n",
    "\n",
    "- **`Retired year`** - Year the plant is taken offline.\n",
    "    - **Additional Note:** This includes mothballed plants - whenever the unit stops operating.\n",
    "\n",
    "\n",
    "- **`Operator`** - The company that operates the project.\n",
    "    - **Additional Note:** Company that performs day-to-day operations and maintenance of the project\n",
    "\n",
    "\n",
    "- **`Operator Name in Local Language / Script`** - Company name that operates the project in the locally used script.\n",
    "    - **Additional Note:** Left blank for local languages use the roman/latin alphabet. Otherwise operator name in local script included if possible\n",
    "\n",
    "\n",
    "- **`Owner`** - The company that directly owns the plant; When owned by a special purpose vehicle, the next level of ownership is provided.\n",
    "\n",
    "\n",
    "- **`Owner Name in Local Language / Script`** - Company name that directly owns the project in the locally used script.\n",
    "    - **Additional Note:** Left blank for local languages that use the roman/latin alphabet. Otherwise direct owner name included in local script if possible.\n",
    "\n",
    "\n",
    "- **`Latitude` (decimal degrees)** - Latitude, assumed WGS84 (Google maps), of the project (not by unit).\n",
    "\n",
    "\n",
    "- **`Longitude` (decimal degrees)** - Longitude, assumed WGS84 (Google maps), of the project (not by unit).\n",
    "\n",
    "\n",
    "- **`Location accuracy` (approximate, exact)** - Location is Approximate based on using nearest city or other information in data sources, or Location is Exact when explicitly provided in data source.\n",
    "\n",
    "\n",
    "- **`City`** - City, town, village, or township (China) where project is located.\n",
    "\n",
    "\n",
    "- **`Local Area`** - County, taluk, or district (China) where project is located .\n",
    "\n",
    "\n",
    "- **`Major Area`** - Prefecture, district, or municipality (China) where project is located.\n",
    "\n",
    "\n",
    "- **`State/Province`** - Subnational unit where project is located.\n",
    "\n",
    "\n",
    "- **`Region`** - Region location based on IRENA definition.\n",
    "\n",
    "\n",
    "- **`GEM Location ID` (L8+5 digits)** - GEM generated ID unique for each project.\n",
    "\n",
    "\n",
    "- **`GEM Phase ID` (G8+5 digits)** - GEM generated ID unique for each phase.\n",
    "\n",
    "\n",
    "- `Other IDs (location)`\n",
    "\n",
    "\n",
    "- `Other IDs (unit/phase)`\n",
    "\n",
    "\n",
    "- **`Wiki URL`** - URL of existing or planned GEM.wiki page.\n",
    "\n",
    "#### B) Regarding the `Status` column - these are the definitions of each project's status:\n",
    "\n",
    "- ***The 'operating' status:*** \n",
    "\n",
    "Commercial operation date achieved.\n",
    "\n",
    "- ***The 'construction' status:*** \n",
    "\n",
    "Equipment installation has begun (not just clearing/roads)\n",
    "\n",
    "- ***The 'development' status:*** \n",
    "\n",
    "Projects that are actively moving forward in seeking governmental approvals, land rights, or financing.\n",
    "\n",
    "- ***The 'announced' status:*** \n",
    "\n",
    "Projects that been publicly reported but have not yet moved actively forward by applying for permits or seeking land, material, or financing. Examples: (1) projects are the potential “Phase 2” at a location where “Phase 1” is currently under development, (2) Projects that are described in long-range company or governmental planning documents.\n",
    "\n",
    "- ***The 'shelved' status:*** \n",
    "\n",
    "Suspension announced, or 2 years with no published updates\n",
    "\n",
    "- ***The 'mothballed' status:*** \n",
    "\n",
    "Disused but not dismantled\n",
    "\n",
    "- ***The 'retired' status:*** \n",
    "\n",
    "Dismantled\n",
    "\n",
    "- ***The 'canceled' status:*** \n",
    "\n",
    "Cancellation announced, or 4 years with no published updates\n",
    "\n",
    "#### C) A note regarding the solar farms capacity cutoff voltage\n",
    "\n",
    "Below are added the capacity cutoff voltages values (the minimum voltage) in **Arab countries, which is any capacity under 10 MWs**, while **the rest of the world cutoff voltage capacity is under 20 MWs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed89fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder of the minimum capacity cutoff voltages stats -\n",
    "display(cutoff_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ba075",
   "metadata": {},
   "source": [
    "***The next steps will be thus:***\n",
    "1. We'll explore the initial data in order to find defects, missing values, duplicates etc. Since we already know the columns are all capitalized, these will be transformed in the Data Preprocessing section.\n",
    "2. We'll preprocess all the data to fix all possible problems we will find during the Data Exploration section.\n",
    "3. Analysis: We'll visualize the data in order to analyze the energy output, average output, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf43df",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0701d7f",
   "metadata": {},
   "source": [
    "# 2. Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b923c1",
   "metadata": {},
   "source": [
    "### 2.1 General exploration\n",
    "\n",
    "Let's begin exploring the `global_solar` DF by having a glance at the first rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b24a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the first 3 rows of the data -\n",
    "display(global_solar.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cb421",
   "metadata": {},
   "source": [
    "And the last rows of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86771511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the first 3 rows of the data\n",
    "display(global_solar.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c996de",
   "metadata": {},
   "source": [
    "*As we already know from the About data explanation section, we will need to lowercase and rename most of the columns in the data to shorter, more simple names.*\n",
    "\n",
    "Looking at the tails of the data, we can already notice a few column that contain NaN values, that we will need to handle in the data preprocessing stage.\n",
    "\n",
    "Let's check the general macro overview of the data using the `info()` method and describe the data's columns using the `describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45e4ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A) Using info() to get a general overview of the data -\n",
    "print('\\033[1m\\x1b[4mGetting a general overview of the DF\\x1b[0m:', end='\\n\\n')\n",
    "print(global_solar.info(), end='\\n\\n')\n",
    "\n",
    "# B) Describing the columns:\n",
    "print('-----------------------------------------------------------------------\\n') # divider\n",
    "# 1. Describing the DF's columns -\n",
    "print('\\033[1m\\x1b[4mDescribing the DF\\'s columns\\x1b[0m:')\n",
    "display(global_solar.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85a9ad",
   "metadata": {},
   "source": [
    "According to the general info() overview of the data, we can notice a few issues:\n",
    "- There are 15 columns containing missing values in the data.\n",
    "- These columns' dtypes should be changed:\n",
    "    - The `start_year` and `retired_year` should be converted to a `int64` dtype, as they represent full years.\n",
    "    - The `Country`, `Region` and `Status` could be converted to the categorical `category` dtype.\n",
    "\n",
    "The numeric columns description gives us the following information:\n",
    "- Looking at the mean of the `Latitude` and `Longitude`, it seems that the majority of the solar projects are located east of the longitude equator and north of the latitude equator line, which is approximated the region between northern Africa and south-western Europe.\n",
    "- There are just 5 solar projects with a fixed retired date, which are forecasted to retire from 2047 to 2058. Meanwhile, 6.3k projects have a start date that has already been reached or is forecasted to, and that looks like there are exactly 3k solar projects with no known activation dates yet in the data. Thing is, as we already know - the `start_year` column is missing for projects that are canceled or shelved, so **there are 3k solar projects that are currently canceled or shelved.**\n",
    "- The average capacity of all the plants in the DF is 87 MWs, while the range vary from 10 MWs to 20k MWs with a standard deviation of about 326.7 MWs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4ca18",
   "metadata": {},
   "source": [
    "### 2.2 Exploring missing values\n",
    "\n",
    "Now we will need to check the missing values in order to find possible connections between them so we could fill them later on.\n",
    "\n",
    "I'll calculate the percentages of missing values in each column of the DF and present the missing values situation in a `missingno` plot that shows the share of non-null values in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637bc37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Printing a guiding note for the plot -\n",
    "print('\\033[1m\\x1b[4mGuiding Note\\x1b[0m: The \\033[1m\\x1b[32mgreen\\x1b[0m horizontal line represents 100% full columns.\\n\\\n",
    "              The \\033[1m\\x1b[33morange\\x1b[0m line represents 50% full columns.\\n\\\n",
    "              The \\033[1m\\x1b[31mred\\x1b[0m line represent the 5% full threshold.')\n",
    "\n",
    "# Plotting horizontal lines representing:\n",
    "plt.axhline(0.5,0,1, c='orange', alpha=0.65, linewidth=1.5); # the full threshold\n",
    "plt.axhline(0.05,0,1, c='red', alpha=0.65, linewidth=1.5); # half-full threshold \n",
    "plt.axhline(1,0,1, c='lightgreen', alpha=0.85, linewidth=3); # less than 5% full threshold\n",
    "\n",
    "# Plotting the share of non-missing values in the data's columns -\n",
    "msno.bar(global_solar, label_rotation=27, fontsize=18)\n",
    "\n",
    "# Plot modifications:\n",
    "plt.title('Share of non-missing values in each column', fontsize=32, pad=20, style='oblique');\n",
    "plt.grid()\n",
    "\n",
    "# Showing plot -\n",
    "plt.show()\n",
    "\n",
    "# Calculating the share of missing values in the DF -\n",
    "print('\\033[1m\\x1b[4mPercentage of missing values from the DF (In a descending order)\\x1b[0m:\\n\\n',\\\n",
    "      round(global_solar.isna().sum() / len(global_solar.isna()) * 100, ndigits=2)\\\n",
    "      .sort_values(ascending=False), sep='');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9932d33c",
   "metadata": {},
   "source": [
    "There are 15 columns containing missing values in the DF.\n",
    "\n",
    "Out of these, *there are some columns that we can overlook:*\n",
    "1. We already know that the \"local name\" columns, i.e. `Operator Name in Local Language / Script` (69.84% missing), `Owner Name in Local Language / Script` (62.89% missing) and `Project Name in Local Language / Script` (46.27% missing) are filled only when there is a local name to these available, so we can ignore these columns missing values.\n",
    "2. Secondly, the `start_year` column is known to have missing values for projects that are canceled or shelved. In the same way, the `Retired year` column, which has a missing values rate of 99.95%, is only filled if the project has a destined and known retirement year.\n",
    "3. The `Other Name(s)` column (82.85% missing) is only presenting alternate project names if there are any. Same goes for the `Other IDs (unit/phase)` (91.9% missing) and `Other IDs (location)` (77.01% missing) columns, which represent other IDs only if there are any.\n",
    "\n",
    "#### *Conclusion:*\n",
    "\n",
    "***The other columns, which will require further handling, are:***\n",
    "- **`Phase Name`:** \n",
    "      contains 57.34% missing values.\n",
    "- **`City`:** \n",
    "      contains 47.39% missing values.\n",
    "- **`Major area (prefecture, district)`:** \n",
    "      contains 42.01% missing values.\n",
    "- **`Operator`:** \n",
    "      contains 35.33% missing values.\n",
    "- **`Local area (taluk, county)`:** \n",
    "      contains 34.37% missing values.\n",
    "- **`Owner`:** \n",
    "      contains 9% missing values.\n",
    "- **`State/Province`:** \n",
    "      contains 6.9% missing values.\n",
    "      \n",
    "***Note:***\n",
    "Although the`Phase Name` column contains missing values, it is worthy of saying that the GEM has the `GEM phase ID` column, stating each unique project's unique phase's unique ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e2664",
   "metadata": {},
   "source": [
    "### 2.3 Searching for duplicates\n",
    "\n",
    "In order to fish for duplicates, we will check both complete duplicates, and duplicates only by project name and countries, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e388c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for complete duplicates -\n",
    "print(f\"Amount of complete duplicates: {global_solar.duplicated().sum()}\")\n",
    "\n",
    "# Checking for partial duplicates where the same projects from a country appear more than once:\n",
    "# 1. Filtering data to include only necesary columns for the check -\n",
    "part_dup = global_solar[['Country', 'Project Name', 'Phase Name']]\n",
    "# 2. Checking for partial duplicates -\n",
    "print(f\"Amount of partial duplicates: {part_dup.duplicated().sum()} \\\n",
    "(which represents {part_dup.duplicated().sum() / len(global_solar):.3%} from the DFs rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0e896",
   "metadata": {},
   "source": [
    "There are no complete duplicates, but it seems like there might be a few partial ones.\n",
    "\n",
    "Let's check why the partial duplicates exist by first looking at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8834c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the duplicates unique projects' names in a variable -\n",
    "dup_proj_names = part_dup[part_dup.duplicated()]['Project Name'].tolist()\n",
    "\n",
    "# Filtering data for rows for these specific projects -\n",
    "display(global_solar[global_solar['Project Name'].isin(dup_proj_names)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae3525",
   "metadata": {},
   "source": [
    "Checking the first duplicate `\"Anhui Lingbi Louzhuang Photovoltaic solar farm\"` project, we can see that although the two first rows seem as the same project at the same year at first sight, they represent 2 separate phases which occurred on the same year in a 2 close towns with almost similar names, that can be supported by the different GEM phase and location IDs and phase location coordinates.\n",
    "\n",
    "Regarding the second duplicate `\"Henan Yuzhou Yuke Photovoltaic Power Co., Ltd.\"` project, we can see much clearer than the first duplicate that these are 2 separate phases, which occurred in different locations in a gap of 2 years between them.\n",
    "\n",
    "#### *Conclusion:*\n",
    "\n",
    "Hence, we will leave the duplicated rows as they were."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108da47d",
   "metadata": {},
   "source": [
    "### 2.4 Conclusions and further steps\n",
    "\n",
    "After the initial data estimation of the raw DF, we've noticed a few issues -\n",
    "- All columns' names are capitalized with spaces between words, also some of the columns names are just too long.\n",
    "\n",
    "\n",
    "- These columns' dtypes should be changed:\n",
    "    - The `start_year` and `retired_year` should be converted to a `int64` dtype, as they represent full years.\n",
    "    - The `Country`, `Region` and `Status` could be converted to the categorical `category` dtype.\n",
    "    \n",
    "    \n",
    "- There are 3k solar projects that are currently canceled or shelved.\n",
    "\n",
    "\n",
    "- There are 15 columns containing missing values, out of which 8 columns are clearly fine to ignore. The columns who require special attention are:\n",
    "    - **`Phase Name`** \n",
    "    - **`City`** \n",
    "    - **`Major area (prefecture, district)`**\n",
    "    - **`Operator`** \n",
    "    - **`Local area (taluk, county)`**\n",
    "    - **`Owner`** \n",
    "    - **`State/Province`** \n",
    "\n",
    "\n",
    "- While fishing for duplicates, we've found that there are 2 partially duplicated rows, that as we checked, weren't actually duplicates at all, they just seemed like it because of the NaN `Phase Name` values. Therefore the data is clear of duplicates.\n",
    "\n",
    "The next steps will be thus:\n",
    "- Modify and change the columns by lowercasing and renaming most of the columns in the data to shorter, more simple names for a cleaner analysis.\n",
    "- Fixing the columns' dtypes.\n",
    "- Check the consistency across the `Country` column trackers, *as the database author mentioned that although it should be ordered, it might lack grouping consistency*.\n",
    "- Handling the remaining 7 missing values columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab527879",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899eee8c",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing\n",
    "\n",
    "We will use this section to fix and prepare the data to analysis.\n",
    "\n",
    "In order to save some time, I'll import a few pre-defined functions of my own for the further work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f12221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a few helpful functions:\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# 1. Defining a function that gets rid of the index:\n",
    "def flat_Idx(df, idx_col, reset_index=True, reorder=False):\n",
    "    '''\n",
    "    The flat_Idx() function is used to flatten a 1-leveled index, \n",
    "    and place the original index values as a regular column before dropping the original index.\n",
    "    \n",
    "    Parameters:\n",
    "    df: The Indexed DataFrame to flatten.\n",
    "    idx_col: The name of the index column. Also the original DataFrame's Index.\n",
    "    reset_index: True by default, resetting the original DataFrame's MultiIndex and dropping the old one.\n",
    "    reorder: Placing the Index column/s at the beginning of the DataFrame. False by default. \n",
    "             Reordered DataFrame is returned, not inplace.\n",
    "    '''\n",
    "    # Salvaging the index column values -\n",
    "    idx = [x for x in df.index]\n",
    "\n",
    "    # re-Creating the originally-index column as a normal column -\n",
    "    df[idx_col] = idx\n",
    "    \n",
    "    # Resetting index (As long as reset_index=True)-\n",
    "    df.reset_index(drop=True,inplace=reset_index)\n",
    "\n",
    "    # loop is ran only if reorder=True:\n",
    "    if reorder==True:\n",
    "        # Storing former-index column' name -\n",
    "        col_name = df.iloc[:,-1].name\n",
    "        # Storing df columns' names -\n",
    "        cols = df.columns\n",
    "        cols = cols.insert(0,col_name)\n",
    "        return df.reindex(columns=cols).iloc[:,:-1] # The iloc removes the original salvaged index column\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# 2. Defining a function that insert last column as first column -\n",
    "def l2f(df):\n",
    "    '''\n",
    "    The last_to_first() function place the last column in the beginning of the DataFrame.\n",
    "    '''\n",
    "    col_name = df.iloc[:,-1].name\n",
    "    cols = df.columns\n",
    "    cols = cols.insert(0,col_name)\n",
    "    return df.reindex(columns=cols).iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5f9b3",
   "metadata": {},
   "source": [
    "## 3.1 Fixing columns' names\n",
    "\n",
    "Since simply lowercasing the columns won't do the whole trick, I'll replace all columns names below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the columns' names -\n",
    "global_solar.columns = ['country', 'project_name', 'phase_name',\n",
    "                        'project_name_local', 'other_names',\n",
    "                        'capacity_mw', 'capacity_rating', 'status', 'start_year',\n",
    "                        'retired_year', 'operator', 'operator_local',\n",
    "                        'owner', 'owner_local', 'latitude',\n",
    "                        'longitude', 'loc_accuracy', 'city', 'local_area',\n",
    "                        'major_area', 'state_prov', 'region',\n",
    "                        'gem_loc_id', 'gem_phase_id', 'other_loc_id',\n",
    "                        'other_unit_phase_id', 'wiki_url']\n",
    "\n",
    "# Printing the new columns' names -\n",
    "print(global_solar.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa99337d",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "\n",
    "The columns names have been fixed.\n",
    "\n",
    "##### ***Below are the new columns' names and their shortened descriptions:***\n",
    "- **`country`** - Country the project is located in (Unabbreviated)\n",
    "\n",
    "\n",
    "- **`project_name`** - Commonly used name.\n",
    " \n",
    "\n",
    "- **`phase_name`** - Common name of the project phase.\n",
    " \n",
    "\n",
    "\n",
    "- **`local_project_name`** - Commonly used name in the locally used script.\n",
    " \n",
    "\n",
    "\n",
    "- **`other_names`** - Alternative names, including the name in local script.\n",
    "\n",
    "\n",
    "- **`capacity_mw`** - Nameplate capacity, or best available data, of the phase in megawatts rounded to whole number.\n",
    " \n",
    "\n",
    "- **`capacity_rating`** - The reported capacity rating, unknown selected if not provided.\n",
    " \n",
    " \n",
    "- **`status`** - *see status definitions in the 'Introduction to the data' section at the start of the report*.\n",
    "\n",
    "\n",
    "- **`start_year`** - Year the project is or is expected to be commissioned.\n",
    " \n",
    " \n",
    "- **`retired_year`** - Year the plant is taken offline.\n",
    " \n",
    " \n",
    "- **`operator`** - The company that operates the project.\n",
    " \n",
    "\n",
    "- **`local_operator`** - Company name that operates the project in the locally used script.\n",
    " \n",
    "\n",
    "- **`owner`** - The company that directly owns the plant; When owned by a special purpose vehicle, the next level of ownership is provided.\n",
    "\n",
    "\n",
    "- **`local_owner`** - Company name that directly owns the project in the locally used script.\n",
    " \n",
    "\n",
    "- **`latitude`** - Latitude, assumed WGS84 (Google maps), of the project (not by unit).\n",
    "\n",
    "\n",
    "- **`longitude`** - Longitude, assumed WGS84 (Google maps), of the project (not by unit).\n",
    "\n",
    "\n",
    "- **`loc_accuracy`** - Location is Approximate based on using nearest city or other information in data sources, or Location is Exact when explicitly provided in data source.\n",
    "\n",
    "\n",
    "- **`city`** - City, town, village, or township (China) where project is located.\n",
    "\n",
    "\n",
    "- **`local_area`** - County, taluk, or district (China) where project is located .\n",
    "\n",
    "\n",
    "- **`major_area`** - Prefecture, district, or municipality (China) where project is located.\n",
    "\n",
    "\n",
    "- **`state_prov`** - Subnational unit where project is located.\n",
    "\n",
    "\n",
    "- **`region`** - Region location based on IRENA definition.\n",
    "\n",
    "\n",
    "- **`gem_loc_id`** - GEM generated ID unique for each project.\n",
    "\n",
    "\n",
    "- **`gem_phase_id`** - GEM generated ID unique for each phase.\n",
    "\n",
    "\n",
    "- `other_loc_id`\n",
    "\n",
    "\n",
    "- `other_unit_phase_id`\n",
    "\n",
    "\n",
    "- **`wiki_url`** - URL of existing or planned GEM.wiki page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7590ad",
   "metadata": {},
   "source": [
    "## 3.2 Fixing data types\n",
    "\n",
    "The data contain wrong dtypes in a few columns:\n",
    "- The `start_year` and `retired_year` will be converted to a `int64` dtype, as they represent full years.\n",
    "- The `country`, `region` and `status` will be converted to the categorical `category` dtype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80028b",
   "metadata": {},
   "source": [
    "#### **3.2.1 The `'start_year'` and `'retired_year'` columns**\n",
    "\n",
    "These columns will be converted to the integer dtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check- Before operation:\n",
    "print(\"Before the operation 'start_year' column's dtype :\", global_solar['start_year'].dtype)\n",
    "print(\"Before the operation 'retired_year' column's dtype :\", global_solar['retired_year'].dtype)\n",
    "\n",
    "# Changing the 'start_year' and 'retired_year' dtypes to Int64 -\n",
    "global_solar['start_year'] = global_solar['start_year'].astype('Int64')\n",
    "global_solar['retired_year'] = global_solar['start_year'].astype('Int64')\n",
    "\n",
    "# Sanity Check- After operation:\n",
    "print(\"\\nAfter the operation 'start_year' column's dtype :\", global_solar['start_year'].dtype)\n",
    "print(\"After the operation 'retired_year' column's dtype :\", global_solar['retired_year'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb6cad",
   "metadata": {},
   "source": [
    "#### *Results:*\n",
    "\n",
    "The `'start_year'` and `'retired_year'` columns dtype has been changed from `float64` to `Int64`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb767f1e",
   "metadata": {},
   "source": [
    "#### **3.2.2 The `country`, `region` and `status` columns**\n",
    "\n",
    "These columns will be converted to a categorical dtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check- Before operation:\n",
    "print(\"Before the operation 'country' column's dtype :\", global_solar['country'].dtype) # for 'country'\n",
    "print(\"Before the operation 'region' column's dtype :\", global_solar['region'].dtype) # for 'region'\n",
    "print(\"Before the operation 'status' column's dtype :\", global_solar['status'].dtype) # for 'status'\n",
    "\n",
    "# Changing the columns' dtypes to category -\n",
    "global_solar['country'] = pd.Categorical(global_solar.country) # for 'country'\n",
    "global_solar['region'] = pd.Categorical(global_solar.region) # for 'region'\n",
    "global_solar['status'] = pd.Categorical(global_solar.status) # for 'status'\n",
    "\n",
    "# Sanity Check- After operation:\n",
    "print(\"\\nProcess being made...\\n\", flush=True)\n",
    "print(\"After the operation 'country' column's dtype :\", global_solar['country'].dtype) # for 'country'\n",
    "print(\"After the operation 'region' column's dtype :\", global_solar['region'].dtype) # for 'region'\n",
    "print(\"After the operation 'status' column's dtype :\", global_solar['status'].dtype) # for 'status'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd594d",
   "metadata": {},
   "source": [
    "#### *Results:*\n",
    "\n",
    "The `'country'` and `'region'` `'status'` columns dtype has been changed from `object` to `category`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf390b",
   "metadata": {},
   "source": [
    "## 3.3 Validating consistency across the `country` column\n",
    "\n",
    "The database author mentioned in the 'About' section notes (added in the data introduction) that the column might lack in consistency.\n",
    "\n",
    "As a consistent alphabetically DF will help us see a more grouped look of the data, we might like to make sure that it is sorted right. We will check it using the `unique()` function, as a consistent column would show an alphabetically ordered output with no misspelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb53d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the unique countries are ordered alphabetically and contain no mispellings -\n",
    "for c in global_solar['country'].unique():\n",
    "    print(c, end=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535ac17",
   "metadata": {},
   "source": [
    "We can notice that the `'country'` column is sorted alphabetically only partly on the first rows, but on the plus side there are no misspelling or partial duplicated country names.\n",
    "\n",
    "To fix the problem we will sort the values alphabetically below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ea508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the data by countries alphabetically, resetting index -\n",
    "global_solar = global_solar.sort_values(by='country').reset_index(drop=True)\n",
    "\n",
    "# Checking the new order of the data -\n",
    "for c in global_solar['country'].unique():\n",
    "    print(c, end=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315bef24",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "\n",
    "The data is now organized by the consistent `country` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e506c",
   "metadata": {},
   "source": [
    "## 3.4 Handling missing values\n",
    "\n",
    "The following columns require missing values intervention:\n",
    "- **`phase_name`** - 57.34% missing values.\n",
    "- **`city`** - 47.39% missing values.\n",
    "- **`major_area`** - 42.01% missing values.\n",
    "- **`operator`** - 35.33% missing values.\n",
    "- **`local_area`** - 34.37% missing values.\n",
    "- **`owner`** - 9% missing values.\n",
    "- **`state_prov`** - 6.9% missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7906c6",
   "metadata": {},
   "source": [
    "### 3.4.1 Checking for connections between missing values\n",
    "\n",
    "Since there are quite an amount of columns containing missing values, a smart step would be to begin by looking for a connection between these columns to others in order to get a lead on how to handle each one of them.\n",
    "\n",
    "Let's begin by checking for connections between missing values using a few `missingno` plots that will plot the nothing. We'll be starting with a correlation heatmap showing the chance of missing values to appear in certain columns if other columns have missing values as well, and a dendrogram showing the appearance dependencies of missing or non-missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539cff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation of missing values appearance between DF's columns -\n",
    "msno.heatmap(global_solar, cmap='seismic_r', label_rotation=30)\n",
    "\n",
    "# Adding a title:\n",
    "plt.title('Correlation map of missing values between columns', fontsize=32, pad=-5);\n",
    "\n",
    "# Showing plot -\n",
    "plt.show()\n",
    "\n",
    "# Printing a divider\n",
    "print('\\n---------------------------------------------------------------------------------------------------------------\\\n",
    "---------------\\n')\n",
    "\n",
    "# Plotting a missing values dendrogram -\n",
    "msno.dendrogram(global_solar, filter='bottom', p=0.99, \\\n",
    "                orientation='left', label_rotation=30, figsize=(14,10))\n",
    "\n",
    "# Plot modifications:\n",
    "plt.title('Co-Appearance dendrogram of missing values in the columns', fontsize=32, pad=15);\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# Showing plot -\n",
    "plt.show()\n",
    "\n",
    "# Printing a divider\n",
    "print('\\n---------------------------------------------------------------------------------------------------------------\\\n",
    "---------------\\n')\n",
    "\n",
    "# Printing stats:\n",
    "oper_cols_miss_chance = global_solar['operator'].isna().sum() / global_solar['operator_local'].isna().sum()\n",
    "oper_cols_miss_overlap = len(global_solar[(global_solar['operator'].isna()) & (global_solar['operator_local'].isna())])\n",
    "operator_missing = global_solar['operator'].isna().sum()\n",
    "print(f'\\033[1m\\x1b[4mSome stats\\x1b[0m:\\n\\n\\\n",
    "Share of rows where missing values appear in both `operator` and `operator_local` columns simultanously \\\n",
    "is {oper_cols_miss_chance:.2%} of the rows.\\n\\\n",
    "The amount of overlapping missing values rows between the two columns is {oper_cols_miss_overlap} rows,\\n\\\n",
    "while he total number of missing values in the `operator` column is {operator_missing}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ac28b",
   "metadata": {},
   "source": [
    "**I'll interpret the key insights from each of the plots separately below.**\n",
    "\n",
    "#### ***Heatmap interpretation:***\n",
    "\n",
    "*Be noted that columns which don't contain missing values were left empty. Also note that we plotted **all** columns including the ones we've decided earlier that we don't need to fill in order to see if we can fill them using other columns.*\n",
    "\n",
    "Looking strictly at our main focus group of missing values columns, the most notable insight is that the `operator` column has a mid-leveled correlation with the `operator_local` column of 0.5, which while adding to the printed additional printed statistics, confirm **half of the rows containing missing values in the `operator_local` column are also missing all `operator` column values**, or to say is the other way - **3290 of the missing values in the `operator` column are also missing in the `operator_local` column, while only 7 of them don't share the overlap**; in 50% of the rows where one is missing - the latter is missing as well.\n",
    "\n",
    "At a little slighter strength, the `phase_name` missing values has a light correlation of 0.3 with the `other_unit_phase_id` column that represents other IDs for than the known IDs.\n",
    "\n",
    "A worthy notices is deserved to the local names' columns (the project, operator and owner local names) which might show strong results in the heatmap, but aren't relevant because they have a huge amount of missing values derived from the fact not every of these parameters even possess a local language name, or a non English one per-se, meaning that the null values probably overlap with the majority of the columns, so their high and distorted correlation is inadmissible to confirm any relation between columns. Also, the `start_year` and `retired_year`have a full correlation of exactly 1, because as we already know from the database author's notes, the `start_year` wasn't included for projects who got canceled or shelved so there's a high chance the same goes for the `retired_year` column because projects who got canceled or shelved can't be retired because they didn't even start, meaning ***we can say all rows who miss both the `start_year` and `retired_year` columns can be considered as shelved or canceled.***\n",
    "\n",
    "#### ***Dendrogram interpretation:***\n",
    "\n",
    "*Note: In order to correctly understand the dendrogram, read it from right to left. The plot is showing the appearance or disappearance correlation of connected columns, meaning that values in the columns connected by a flat line at the right are directly appearing or disappearing when the other leaf connected columns' values appear or disappear, orderly, and vice versa, while the ones connected at the most left-sided leaves have the slightest connection of correlation on appearance. In addition, the plot is filtered to include only the original 15 columns who contain missing values, disregarding the full columns.*\n",
    "\n",
    "We can notice that the strongest connections of co-appearance between the columns are those of the `operator_local` and `owner_local`, which makes sense because we already know that most countries don't have local names except ones who aren't of roman alphabet origins, and appear or disappear mostly on the same occasions. Most of the other connections are pretty trivial.\n",
    "\n",
    "We've already seen a few light to middle connections in between the columns, but for now it's not enough for most of the columns missing values to state of an actual connection in appearance of missing values. Therefore, in order to be certain of a lack of connection, I'll plot a matrix plot that checks the precise location of missing values in the DF, I'll keep it unordered in purpose of seeing directly if there is a connection between missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85cde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the nullity matrix -\n",
    "msno.matrix(global_solar, label_rotation=30)\n",
    "\n",
    "# Plot modifications:\n",
    "plt.title('Nullity matrix of the exact location of non-missing values', fontsize=32, pad=15);\n",
    "plt.axhline(635,0.435,0.492, c='orange', alpha=0.25, linewidth=75);\n",
    "plt.axvline(8.5,0,1, c='red', alpha=0.15, linewidth=105);\n",
    "\n",
    "# Showing plot -\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073fcbfa",
   "metadata": {},
   "source": [
    "*Plot guide notes: The sparkline on the right shows the amount of columns containing missing values in each row, and the plot's black lines each represent non-null values for each rows at each certain column, while their absence represent the opposite.*\n",
    "\n",
    "By now we can now clearly confirm that ***there aren't any established connections between the appearance of missing values in any of the group specific columns to the appearance of others***. \n",
    "\n",
    "We can see a supporting act to our previous statement while looking at all rows who miss both the `start_year` and `retired_year` values in the faded red marker on the plot, as we can clearly see both are appearing or missing in the *exact* same rows.\n",
    "\n",
    "On the other hand, we can see both in the plot and the sparkline frequency that the missing values appear in what seems to be the same countries in a pretty similar way, that can be seen while looking at different patterns in all columns from top to bottom, as we already know the data is ordered by the countries. $For$ $example$ *look horizontally at the rows' patterns in the drawn faded orange boxes in the middle of the DF plot*.\n",
    "\n",
    "#### **Conclusions regarding missing values correlation:**\n",
    "\n",
    "As I stated 3 paragraphs above, except from a few connections between the columns which were left empty in purpose, there aren't any established connections between the appearance of missing values in any of the columns to the appearance of others, ***meaning we will need to handle each column individually***. Nevertheless, the strongest insight came from the matrix plot, showing that ***there might be a patterned connection between missing values in specific countries***, a fact which we can use while handling the missing values in the next steps.\n",
    "\n",
    "*Also*, after fixing the data we could enrich the data with a new boolean column to state whether solar projects are shelved or canceled by simply checking if both `start_year` and `retired_year` are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e1cff",
   "metadata": {},
   "source": [
    "### 3.4.2 Handling the locations columns\n",
    "\n",
    "We will use the coordinates columns `latitude` and `longitude` to fill the following columns:\n",
    "- `city` (rep. by **city**) - 47.39% missing values.\n",
    "- `local_area` (rep. by **county**) - 34.37% missing values.\n",
    "- `major_area` (rep. by **state_district**) - 42.01% missing values.\n",
    "- `state_prov` (rep. by **state**) - 6.9% missing values.\n",
    "\n",
    "To do that action we'll use the GeoPy mapping library which converts coordinates to addresses, while we will extract each missing column's max amounts of specific values we can get. In columns where the location accuracy (`loc_accuracy`) is 'exact' and fully accurate, we'll fill all fields we could get, while in the 'approximate' accurate ones we'll fill all missing values except the `city` to keep the data integrity right.\n",
    "\n",
    "We'll begin by creating the `'coordinates'` column that will be concatenating the `latitude` and `longitude` columns in a geographical coordinates format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenting latitude and longitude to create the 'coordinates' column -\n",
    "global_solar['coordinates'] = global_solar['latitude'].map(str) + ', ' + global_solar['longitude'].map(str)\n",
    "\n",
    "# Checking the data's values format and dtype -\n",
    "print('`coordinates` column values\\' format: ', global_solar['coordinates'][0])\n",
    "print('`coordinates` column values\\' dtype: ', type(global_solar['coordinates'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ea09fc",
   "metadata": {},
   "source": [
    "Now that the `'coordinates'` column has been created, The next step will be to create the `reverse_mapping()` function that will reverse-map each row's coordinates according to the given query parameter of each of the necessary columns and add it to an interior variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the `reverse_geocoding()` function -\n",
    "def reverse_geocoding(coordinates, q='display_name', INA='None', delay=0.00015, lang='en'):\n",
    "    '''\n",
    "    Function is used for geocode address mapping of latitude and longitude values;\n",
    "    Simply saying - for converting coordinates to addresses.\n",
    "    \n",
    "    How it works:\n",
    "    The function map each of the coordinates values and return the query item (i.e. `q` parameter) \n",
    "    for each coordinates pair. After that the value is appended to an exterior variable \n",
    "    that will be named `addresses`.\n",
    "    \n",
    "    Parameters:\n",
    "    coordinates: The iterated Series or DataFrame column containing coordinates.\n",
    "    q: The queried parameter; can be either one of the values available at the list below. \n",
    "       Set to 'display_name' by default.\n",
    "    INA: Stands for 'item not available', 'None' by default.\n",
    "       Set the returned values if `q` item is not found for a row.\n",
    "    delay: Set to 0.015 seconds by default. Parameter is controlling the delay between \n",
    "       mapping requests to the API.\n",
    "    lang: Set to 'en' by default. Control translation language of mapped data, that is \n",
    "       translated to English by default.\n",
    "       It's possible to pass two letters country codes (ISO 639-1) to change \n",
    "       the values language.\n",
    "    \n",
    "    Query parameters - options:\n",
    "    place_id, licence, osm_type, osm_id, lat, lon, display_name, address, suburb, city, \n",
    "    state_district, state, postcode, country, country_code, boundingbox\n",
    "    '''\n",
    "    # Looping over `coordinates` to map the `q` item:\n",
    "    for val in coordinates:\n",
    "        time.sleep(delay)\n",
    "        location = geolocator.reverse(val, language=lang)\n",
    "        if location != None:\n",
    "            addresses.append(location.raw.get('address').get(q))\n",
    "        else:\n",
    "            addresses.append(INA)\n",
    "                \n",
    "# Creating a Geolocator mapping object -\n",
    "geolocator = Nominatim(user_agent=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51453a5d",
   "metadata": {},
   "source": [
    "Now that the preparations are set, we can start mapping all missing values.\n",
    "\n",
    "The process will be done for each column's missing values separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9fe9e",
   "metadata": {},
   "source": [
    "<div style=\"border:solid black 2px; padding: 10px\"> <h1 style=\"color:red; margin-bottom:20px\">A strong notice!</h1>\n",
    "    \n",
    "    \n",
    "<p style=\"font-size: 16px\">Since the map API takes time to process each request and cannot receive too many parsing requests simultanously, <strong>the mapping process will take some time to map all data with the added delay between each request (Averagely 75:16 minutes).</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76cdcf2",
   "metadata": {},
   "source": [
    "#### $1.$ Mapping and filling `state/prov`:\n",
    "\n",
    "First we'll create a filtered DF containing all rows where the column has a missing values, and then we will begin mapping the state / province column values, ***process is expected to take around 5-6 minutes (5:23 mins average)***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5fde7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A) Filtering data to rows where the location accuracy is set to 'exact', that contain `state_prov` missing values -\n",
    "locs_state = global_solar[(global_solar['state_prov'].isna())]\n",
    "\n",
    "# Printing the amount of missing values' rows to iterate over the function -\n",
    "print(f'Amount of missing rows: {len(locs_state)}', end='\\n\\n')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Mapping states from `coordinates`:\n",
    "# 1. Creating a blank 'addresses' variable -\n",
    "addresses = []\n",
    "\n",
    "## Counting action time\n",
    "start = perf_counter()\n",
    "print(f'Starting timer at: {start}')\n",
    "\n",
    "# 2. Mapping all missing `state_prov` values translated to English -\n",
    "reverse_geocoding(locs_state['coordinates'], q='state')\n",
    "\n",
    "## Printing total action time\n",
    "end = perf_counter()\n",
    "print(f'Stopping timer at: {end}\\n\\\n",
    "Execution time - {end - start}')\n",
    "\n",
    "# 3. Saving the addresses to a unique var -\n",
    "missing_states = addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee5dbfe",
   "metadata": {},
   "source": [
    "Now that we have all missing values in the `missing_states` variable, we'll use it to fill all missing values in the `state_prov` column by simply overwriting it, since the filtered DF had originally only null values in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e582e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {locs_state['state_prov'].isna().sum()}\")\n",
    "\n",
    "# Filling missing values in the `state_prov` column\n",
    "# by pasting the `missing_states` mappeing list variable on it -\n",
    "locs_state['state_prov'] = missing_states\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {locs_state['state_prov'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83cfae",
   "metadata": {},
   "source": [
    "We mapped 252 locations out of the 647 missing values in the `'state_prov'` column. Not all countries are divided to states or provinces so it should be satisfying enough.\n",
    "\n",
    "Now we need to fill the original `global_solar` DF with the values we've filled the `locs_state` interim DF.\n",
    "\n",
    "**To start with it, we'll do the following:**\n",
    "1. In this stage we'll:\n",
    "    - Convert the `locs_state` filtered DF and filter it to include only the `'state_prov'` column. We'll flatten the index to a column while keeping the old index.\n",
    "    - Create another filtered DF of all non-missing `'state_prov'` column rows from the original `global_solar` DF and flatten the index to a column while keeping the original index without any resettings.\n",
    "2. We'll use both `'index'` columns as merging keys to join both DFs to one series, and then we'll overwrite the original column with the merged `state_prov` column of the merged filtered DFs.\n",
    "\n",
    "I'll define the following `df2s2df()` function to automate the process for the rest of the process and for the next columns filling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc427a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the `df2s2df()` function to help with automation -\n",
    "def df2s2df(df, col):\n",
    "    '''\n",
    "    Function automate the process of preparing DataFrames to missing values filling.\n",
    "    \n",
    "    Parameters:\n",
    "    df: Iterated DataFrame.\n",
    "    col: The column that is destined to be filled in later process.\n",
    "    '''\n",
    "    # A) Taking care of the `df` data pre-merger:\n",
    "    # 1. Converting the `col` column from the `df` to a Series -\n",
    "    df = df[col]\n",
    "\n",
    "    # 2. Converting the Series to a DataFrame -\n",
    "    ret_df = pd.DataFrame(data=df)\n",
    "\n",
    "    # 3. Flattening index to a column -\n",
    "    flat_Idx(ret_df,'index',reset_index=False)\n",
    "\n",
    "    # 4. Using l2f() to insert index column as first column -\n",
    "    ret_df = l2f(ret_df)\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350acd5",
   "metadata": {},
   "source": [
    "We'll begin ***stage one*** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Taking care of the `locs_state` data pre-merger:\n",
    "# 1. Conducting automation process using the df2s2df() function -\n",
    "locs_state = df2s2df(locs_state,'state_prov')\n",
    "\n",
    "# 2. Check DF's new look -\n",
    "print(f'Amount of rows in the missing values DF: {len(locs_state)}')\n",
    "display(locs_state.head(3))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Create the `non_miss_rows` DF pre-merger,\n",
    "#    DF will contain all non-missing values of the `state_prov` column and an 'index' column:\n",
    "# 1. Making a filtered DF of all non-missing values on the `state_prov` column on the original DF -\n",
    "non_miss_rows = global_solar[~(global_solar['state_prov'].isna())]\n",
    "\n",
    "# 2. Conducting automation process using the df2s2df() function -\n",
    "non_miss_rows = df2s2df(non_miss_rows, 'state_prov')\n",
    "\n",
    "# 3. Check DF's new look -\n",
    "print(f'\\nAmount of rows in the non-missing values DF: {len(non_miss_rows)}')\n",
    "display(non_miss_rows.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577463c",
   "metadata": {},
   "source": [
    "The 2 filtered DFs were created. Now we need to merge them using the `append()` function and overwrite the original column in the basic DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b64cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Appending both filt DFs so that they'll merge to cover all original DF's rows:\n",
    "# 1. Appending both filt DFs together -\n",
    "state_col = locs_state.append(non_miss_rows)\n",
    "\n",
    "# 2. Sorting data by the 'index' column -\n",
    "state_col.sort_values('index', inplace=True)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Filling missing values by \n",
    "#    Overwriting the `state_prov` column by the merged filt DF's correlating column:\n",
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['state_prov'].isna().sum()}\")\n",
    "\n",
    "# Filling the mapped states into the original column by overwriting it -\n",
    "global_solar['state_prov'] = state_col['state_prov']\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['state_prov'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2b9e2",
   "metadata": {},
   "source": [
    "The reverse mapping process mapped 252 locations out of the 647 missing values in the `state_prov` column, and the mapped locations were filled into the DF.\n",
    "\n",
    "Now we will fill the rest of the missing values with the value 'unavailable', because either the countries aren't divided to states or the data just couldn't be mapped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13306a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['state_prov'].isna().sum()}\")\n",
    "\n",
    "# Filling remaining missing values with 'unknown' -\n",
    "global_solar['state_prov'] = global_solar['state_prov'].fillna('Unavailable')\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['state_prov'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b191ce",
   "metadata": {},
   "source": [
    "*Result:*\n",
    "\n",
    "We were able to map and fill 252 locations out of the 647 missing values in the `state_prov` column. The rest of the missing values were filled with the value `'Unavailable'`. Column is completely filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f23a32",
   "metadata": {},
   "source": [
    "#### $2.$ Mapping and filling `major_area`:\n",
    "\n",
    "First we'll create a filtered DF containing all rows where the column has a missing values, and then we will begin mapping the district column values, ***process is expected to take around 26-28 minutes (26:50 mins average):***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Filtering data to rows that contain `major_area` missing values -\n",
    "locs_dist = global_solar[(global_solar['major_area'].isna())]\n",
    "print(f'Amount of missing rows: {len(locs_dist)}', end='\\n\\n')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Mapping districts from `coordinates`:\n",
    "# 1. Creating a blank 'addresses' variable -\n",
    "addresses = []\n",
    "\n",
    "## Counting action time\n",
    "start = perf_counter()\n",
    "print(f'Starting timer at: {start}')\n",
    "\n",
    "# 2. Mapping all missing `major_area` values translated to English -\n",
    "reverse_geocoding(locs_dist['coordinates'], q='state_district', delay=0.00000000000000000000000015)\n",
    "\n",
    "## Printing total action time\n",
    "end = perf_counter()\n",
    "print(f'Stopping timer at: {end}\\n\\\n",
    "Execution time - {end - start}')\n",
    "\n",
    "# 3. Saving the addresses to a unique var -\n",
    "missing_dist = addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432723f6",
   "metadata": {},
   "source": [
    "Now that we have all missing values in the `missing_states` variable, we'll use it to fill all missing values in the `major_area` column by simply overwriting it, since the filtered DF had originally only null values in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df77507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {locs_dist['major_area'].isna().sum()}\")\n",
    "\n",
    "# Filling missing values in the `major_area` column\n",
    "# by pasting the `missing_dist` mapping list variable on it -\n",
    "locs_dist['major_area'] = missing_dist\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {locs_dist['major_area'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f572430",
   "metadata": {},
   "source": [
    "We mapped 582 locations out of the 3920 missing values in the `'major_area'` column.\n",
    "\n",
    "Now we need to fill the original `global_solar` DF with the values we've filled the `locs_dist` interim DF.\n",
    "\n",
    "**To start with it, we'll conduct the same process that was done with last columns - use the `df2s2df()` function to automate the process, append the filtered DFs and overwrite the missing values column.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512532b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Taking care of the `locs_dist` data pre-merger:\n",
    "# Conducting automation process using the df2s2df() function -\n",
    "locs_dist = df2s2df(locs_dist,'major_area')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Create the `non_miss_rows` DF pre-merger,\n",
    "#    DF will contain all non-missing values of the `major_area` column and an 'index' column:\n",
    "# 1. Making a filtered DF of all non-missing values on the `major_area` column on the original DF -\n",
    "non_miss_rows = global_solar[~(global_solar['major_area'].isna())]\n",
    "\n",
    "# 2. Conducting automation process using the df2s2df() function -\n",
    "non_miss_rows = df2s2df(non_miss_rows, 'major_area')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# C) Appending both filt DFs so that they'll merge to cover all original DF's rows:\n",
    "# 1. Appending both filt DFs together -\n",
    "dist_col = locs_dist.append(non_miss_rows)\n",
    "\n",
    "# 2. Sorting data by the 'index' column -\n",
    "dist_col.sort_values('index', inplace=True)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# D) Filling missing values by \n",
    "#    Overwriting the `major_area` column by the merged filt DF's correlating column:\n",
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['major_area'].isna().sum()}\")\n",
    "\n",
    "# Filling the mapped states into the original column by overwriting it -\n",
    "global_solar['major_area'] = dist_col['major_area']\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['major_area'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b88eb3",
   "metadata": {},
   "source": [
    "Now we will fill the rest of the missing values with the value 'unavailable', because the data just couldn't be mapped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['major_area'].isna().sum()}\")\n",
    "\n",
    "# Filling remaining missing values with 'unknown' -\n",
    "global_solar['major_area'] = global_solar['major_area'].fillna('Unavailable')\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['major_area'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c4d55",
   "metadata": {},
   "source": [
    "*Result:*\n",
    "\n",
    "We were able to map and fill 582 locations out of the 3920 missing values in the `major_area` column. The rest of the missing values were filled with the value `'Unavailable'`. Column is filled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c853dea",
   "metadata": {},
   "source": [
    "#### $3.$ Mapping and filling `local_area`:\n",
    "\n",
    "First we'll create a filtered DF containing all rows where the column has a missing values, and then we will begin mapping the county column values, ***process is expected to take around 26-27 minutes (26:43 mins average)***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) # Filtering data to rows that contain `local_area` missing values -\n",
    "locs_local = global_solar[(global_solar['local_area'].isna())]\n",
    "print(f'Amount of missing rows: {len(locs_local)}', end='\\n\\n')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Mapping districts from `coordinates`:\n",
    "# 1. Creating a blank 'addresses' variable -\n",
    "addresses = []\n",
    "\n",
    "## Counting action time\n",
    "start = perf_counter()\n",
    "print(f'Starting timer at: {start}')\n",
    "\n",
    "# 2. Mapping all missing `local_area` values translated to English -\n",
    "reverse_geocoding(locs_local['coordinates'], q='county', delay=0.000000000015)\n",
    "\n",
    "## Printing total action time\n",
    "end = perf_counter()\n",
    "print(f'Stopping timer at: {end}\\n\\\n",
    "Execution time - {end - start}')\n",
    "\n",
    "# 3. Saving the addresses to a unique var -\n",
    "missing_county = addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd5aa9",
   "metadata": {},
   "source": [
    "Now that we have all missing values in the `missing_county` variable, we'll use it to fill all missing values in the `local_area` column by simply overwriting it, since the filtered DF had originally only null values in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39956a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {locs_local['local_area'].isna().sum()}\")\n",
    "\n",
    "# Filling missing values in the `local_area` column\n",
    "# by pasting the `missing_county` mapping list variable on it -\n",
    "locs_local['local_area'] = missing_county\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {locs_local['local_area'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3a0b6",
   "metadata": {},
   "source": [
    "We mapped 1309 locations out of the 3207 missing values in the `'local_area'` column.\n",
    "\n",
    "Now we need to fill the original `global_solar` DF with the values we've filled the `locs_local` interim DF.\n",
    "\n",
    "**To start with it, we'll conduct the same process that was done with last columns - use the `df2s2df()` function to automate the process, append the filtered DFs and overwrite the missing values column.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce3088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Taking care of the `locs_local` data pre-merger:\n",
    "# Conducting automation process using the df2s2df() function -\n",
    "locs_local = df2s2df(locs_local,'local_area')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Create the `non_miss_rows` DF pre-merger,\n",
    "#    DF will contain all non-missing values of the `local_area` column and an 'index' column:\n",
    "# 1. Making a filtered DF of all non-missing values on the `local_area` column on the original DF -\n",
    "non_miss_rows = global_solar[~(global_solar['local_area'].isna())]\n",
    "\n",
    "# 2. Conducting automation process using the df2s2df() function -\n",
    "non_miss_rows = df2s2df(non_miss_rows, 'local_area')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# C) Appending both filt DFs so that they'll merge to cover all original DF's rows:\n",
    "# 1. Appending both filt DFs together -\n",
    "local_col = locs_local.append(non_miss_rows)\n",
    "\n",
    "# 2. Sorting data by the 'index' column -\n",
    "local_col.sort_values('index', inplace=True)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# D) Filling missing values by \n",
    "#    Overwriting the `local_area` column by the merged filt DF's correlating column:\n",
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['local_area'].isna().sum()}\")\n",
    "\n",
    "# Filling the mapped states into the original column by overwriting it -\n",
    "global_solar['local_area'] = local_col['local_area']\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['local_area'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9eb2c8",
   "metadata": {},
   "source": [
    "Now we will fill the rest of the missing values with the value 'unavailable', because the data just couldn't be mapped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f866236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['local_area'].isna().sum()}\")\n",
    "\n",
    "# Filling remaining missing values with 'unknown' -\n",
    "global_solar['local_area'] = global_solar['local_area'].fillna('Unavailable')\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['local_area'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78254fb",
   "metadata": {},
   "source": [
    "*Result:*\n",
    "\n",
    "We were able to map and fill 1309 locations out of the 3207 missing values in the `local_area` column. The remaining 1898 values were filled with the value `'Unavailable'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7e8e0",
   "metadata": {},
   "source": [
    "#### $4.$ Mapping and filling accurate `city`:\n",
    "\n",
    "For `city`, we will only fill the values for rows with an `exact` location accuracy coordinates to keep data integrity. **The rows where location accuracy is 'approximate' will be left blank**.\n",
    "\n",
    "First we'll create a filtered DF containing all rows where the column has a missing values, and then we will begin mapping the city column values, ***process is expected to take around 16-17 minutes (16:20 mins average)***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8103664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Filtering data to rows where the location accuracy is set to 'exact', that contain `city` missing values -\n",
    "exact_locs_city = global_solar[(global_solar['city'].isna()) & (global_solar['loc_accuracy']=='exact')]\n",
    "print(f'Amount of missing rows: {len(exact_locs_city)}', end='\\n\\n')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Mapping cities from `coordinates`:\n",
    "# 1. Creating a blank 'addresses' variable -\n",
    "addresses = []\n",
    "\n",
    "## Counting action time\n",
    "start = perf_counter()\n",
    "print(f'Starting timer at: {start}')\n",
    "\n",
    "# 2. Mapping all accurate missing `city` values translated to English -\n",
    "reverse_geocoding(exact_locs_city['coordinates'], q='county')\n",
    "\n",
    "## Printing total action time\n",
    "end = perf_counter()\n",
    "print(f'Stopping timer at: {end}\\n\\\n",
    "Execution time - {end - start}')\n",
    "\n",
    "# 3. Saving the addresses to a unique var -\n",
    "missing_city = addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce641a23",
   "metadata": {},
   "source": [
    "Now that we have all missing values in the `missing_city` variable, we'll use it to fill all missing values in the `city` column by simply overwriting it, since the filtered DF had originally only null values in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a6a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {exact_locs_city['city'].isna().sum()}\")\n",
    "\n",
    "# Filling missing values in the `city` column\n",
    "# by pasting the `missing_city` mapping list variable on it -\n",
    "exact_locs_city['city'] = missing_city\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {exact_locs_city['city'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46abc272",
   "metadata": {},
   "source": [
    "We mapped 1618 locations out of the 1964 missing values in the `'city'` column.\n",
    "\n",
    "Now we need to fill the original `global_solar` DF with the values we've filled the `exact_locs_city` interim DF.\n",
    "\n",
    "**To start with it, we'll conduct the same process that was done with last columns - use the `df2s2df()` function to automate the process, append the filtered DFs and overwrite the missing values column.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9addb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Taking care of the `exact_locs_city` data pre-merger:\n",
    "# Conducting automation process using the df2s2df() function -\n",
    "exact_locs_city = df2s2df(exact_locs_city,'city')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Create the `non_miss_rows` DF pre-merger,\n",
    "#    DF will contain all non-missing values of the `city` column and an 'index' column:\n",
    "# 1. Making a filtered DF of all non-missing values on the `city` column on the original DF -\n",
    "non_miss_rows = global_solar[~(global_solar['city'].isna()) | \\\n",
    "                             ((global_solar['city'].isna()) & (global_solar['loc_accuracy']=='approximate'))]\n",
    "\n",
    "# 2. Conducting automation process using the df2s2df() function -\n",
    "non_miss_rows = df2s2df(non_miss_rows, 'city')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# C) Appending both filt DFs so that they'll merge to cover all original DF's rows:\n",
    "# 1. Appending both filt DFs together -\n",
    "city_col = exact_locs_city.append(non_miss_rows)\n",
    "\n",
    "# 2. Sorting data by the 'index' column -\n",
    "city_col.sort_values('index', inplace=True)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# D) Filling missing values by \n",
    "#    Overwriting the `city` column by the merged filt DF's correlating column:\n",
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['city'].isna().sum()}\")\n",
    "\n",
    "# Filling the mapped states into the original column by overwriting it -\n",
    "global_solar['city'] = city_col['city']\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['city'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27497d9e",
   "metadata": {},
   "source": [
    "Now we will fill the rest of the missing values with the value 'unavailable', because the data just couldn't be mapped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1496b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['city'].isna().sum()}\")\n",
    "\n",
    "# Filling remaining missing values with 'unknown' -\n",
    "global_solar['city'] = global_solar['city'].fillna('Unavailable')\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['city'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472abc9",
   "metadata": {},
   "source": [
    "*Result:*\n",
    "\n",
    "We were able to map and fill 1618 exactly accurate locations out of the 1964 missing values with an `'exact'` location accuracy, and from a total amount of 4422 missing values in the `city` column itself. The rest of the missing values were filled with the value `'Unavailable'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9dfd7",
   "metadata": {},
   "source": [
    "### Handling `operator` (35.33% missing values)\n",
    "\n",
    "Since we already have the local language `operator` names at our disposal, we'll create a variable containing a query where `operator` is missing and `operator_local` is not.\n",
    "\n",
    "We'll translate the rows where the local name wasn't missing to fill the operator's name for each project row available, **while the intranslatable rows will be left as null values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adbd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Creating counter for rows where \n",
    "# the `operator` name is available only on the local language (`operator_local`) -\n",
    "local_op = global_solar[(global_solar['operator'].isna()) & (global_solar['operator_local'].notna())]\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Filling missing values where operator local name isn't missing and operator English name does:\n",
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['operator'].isna().sum()}\")\n",
    "\n",
    "# Printing amount of queried missing values that could be translated - \n",
    "print(f'The query has found that {len(local_op)} rows are fitting for translation')\n",
    "# Translating and filling available values -\n",
    "for idx in local_op.index:\n",
    "    local_name = local_op.loc[idx, 'operator_local']\n",
    "    global_solar.loc[idx, 'operator'] = ts.google(local_name)\n",
    "    print('\\n.')\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['operator'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d94f05",
   "metadata": {},
   "source": [
    "Resulting from the process, 7 rows were translated from the local name column. The rest of the values will be filled with 'Unknown':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0476eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['operator'].isna().sum()}\")\n",
    "\n",
    "# Filling remaining missing values with 'unknown' -\n",
    "global_solar['operator'] = global_solar['operator'].fillna('Unknown')\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['operator'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65003d",
   "metadata": {},
   "source": [
    "*Result:*\n",
    "\n",
    "We've managed to translate 7 rows from the `operator_local` foreign language/s and fill their missing `operator` column values. The other 3290 rows were filled as `'Unknown'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e3007",
   "metadata": {},
   "source": [
    "### Handling `owner` (9% missing values)\n",
    "\n",
    "Same goes for the `owner` column, which contains a local name column as well. We'll translate the rows where the local name wasn't missing to fill the owner's name for each project row available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38488a41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating counter for rows where \n",
    "# the `owner` name is available only on the local language (`owner_local`) -\n",
    "local_owner = global_solar[(global_solar['owner'].isna()) & (global_solar['owner_local'].notna())]\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Filling missing values where owner local name isn't missing and owner English name does:\n",
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['owner'].isna().sum()}\")\n",
    "\n",
    "# Printing amount of queried missing values that could be translated -\n",
    "print(f'The query has found that {len(local_owner)} rows are fitting for translation')\n",
    "# Translating and filling available values -\n",
    "for idx in local_owner.index:\n",
    "    local_name = local_owner.loc[idx, 'owner_local']\n",
    "    global_solar.loc[idx, 'owner'] = ts.google(local_name)\n",
    "    print('\\n.')\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['owner'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45138cad",
   "metadata": {},
   "source": [
    "Resulting from the process, one row was translated from the local name column. The rest of the values will be filled with 'Unknown':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b73cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['owner'].isna().sum()}\")\n",
    "\n",
    "# Filling remaining missing values with 'unknown' -\n",
    "global_solar['owner'] = global_solar['owner'].fillna('Unknown')\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['owner'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f29d7",
   "metadata": {},
   "source": [
    "*Result:*\n",
    "\n",
    "We were only able to fill one single row of `owner` using the local name column. The other 839 were filled s `'Unknown'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b1318",
   "metadata": {},
   "source": [
    "### Handling `phase_name` (57.34% missing values)\n",
    "\n",
    "There was no accurate data neither on the raw xlsx database file nor the GEM website whether the column relates to a progressive phase, i.e. a stage of development, or to electrical phases, as their data sources regarding our data are pretty vague about that column.\n",
    "\n",
    "Nevertheless, the most doable option of filling the `phase_name` column is to use the GEM phase IDs (i.e. `gem_phase_id`), which although it's nice having them on the data, don't have any records on their sources to parse from, so that column is useless for us as well.\n",
    "\n",
    "The only hope we have for filling a part of the column is the `other_unit_phase_id` column, which include other phases or units IDs, so our plan will be thus:\n",
    "1. Since the filler column contains both phases and units IDs, we'll need to check the unique \"other IDs\" to know what values to ignore (e.g. ones that represent units, which aren't relevant). We'll compare them to the unique `phase_name` we already have in order to see what fits to be filled.\n",
    "2. If there are any suitable, we'll fill the `phase_name` with them. ***The remaining of the column will be left as it is to protect data integrity.***\n",
    "\n",
    "We'll begin by observing the unique values of the `phase_name` and the `other_unit_phase_id` columns to find a common base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dfc40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Variable of all unique `phase_name` values -\n",
    "phase_name_uniq = global_solar[(~global_solar['phase_name'].isna())]['phase_name'].unique()\n",
    "\n",
    "# Print unique values where not missing -\n",
    "print(f'\\033[1m\\x1b[4mUnique values of the `phase_name` column\\x1b[0m:')\n",
    "display(phase_name_uniq)\n",
    "\n",
    "# Variable of all unique `other_unit_phase_id` values where `phase_name` is missing -\n",
    "phases = global_solar[(global_solar['phase_name'].isna()) & \\\n",
    "                      global_solar['other_unit_phase_id'].notna()]['other_unit_phase_id'].unique()\n",
    "\n",
    "# Print unique values where missing -\n",
    "print(f'\\033[1m\\x1b[4mUnique values of the `other_unit_phase_id` column\\x1b[0m:')\n",
    "display(phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd022e7f",
   "metadata": {},
   "source": [
    "We can notice that most of the IDs are meaningless, but the only-numeric ones do fit the profile and look similar to the `phase_name` approximate format. The other IDs look too encrypted to be anything but an encrypted ID, and a few rows just contain notes about overlapping area with other solar farms.\n",
    "\n",
    "Hence, the only option that we could think about out of those is filling missing values only for rows with numeric-only `other_unit_phase_id`, we'll start by scraping only the numeric string from the unique values list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931aeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that \n",
    "# could extract the only-numeric items from the unique values list -\n",
    "def numeric_app(list, counter):\n",
    "    '''\n",
    "    The `numeric_app()` function will append only numeric values from a string type list or Series to the counter.\n",
    "    \n",
    "    Parameters:\n",
    "    list: The iterated list, Series.\n",
    "    counter: An empty counter variable outside the function.\n",
    "    '''\n",
    "    for i in list:\n",
    "        if str(i).isdigit() == True:\n",
    "            counter.append(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Creating a counter for the missing values rows -\n",
    "filled_IDs = []\n",
    "\n",
    "# Finding rows where the 'other ID' contains only numeric characters -\n",
    "numeric_app(phases, filled_IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c09f2",
   "metadata": {},
   "source": [
    "Now that we've pulled all numeric values from the column, we'll use the same methods we used while filling the null location columns' values to fill the original DF's `phase_name` values that we've managed to find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5662cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A) Create the `filling_var` DF pre-merger, DF will contain all missing values of the `phase_name` column\n",
    "#    that have an 'other ID' value that is in the `filled_IDs` variable, and an 'index' column:\n",
    "# 1. Finding rows where `phase_name` is null\n",
    "#    and the other ID is in the `filled_IDs` variable -\n",
    "filling_var = global_solar[(global_solar['phase_name'].isna()) & \\\n",
    "                           global_solar['other_unit_phase_id'].isin(filled_IDs)]\n",
    "\n",
    "# 2. Automated 'DF >> Series >> DF' process -\n",
    "filling_var = df2s2df(filling_var, 'other_unit_phase_id')\n",
    "\n",
    "# 3. Changing columns' names -\n",
    "filling_var.columns = ['index', 'phase_name']\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Create the `non_miss_rows` DF pre-merger,\n",
    "#    DF will contain all non-missing values of the `phase_name` column and an 'index' column:\n",
    "# 1. Making a filtered DF of all non-missing values on the `phase_name` column on the original DF -\n",
    "non_miss_rows = global_solar[~(global_solar['phase_name'].isna())]\n",
    "\n",
    "# 2. Conducting automation process using the df2s2df() function -\n",
    "non_miss_rows = df2s2df(non_miss_rows, 'phase_name')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# C) Appending both filt DFs so that they'll merge to cover all original DF's rows:\n",
    "# 1. Appending both filt DFs together -\n",
    "phase_col = filling_var.append(non_miss_rows)\n",
    "\n",
    "# 2. Sorting data by the 'index' column -\n",
    "phase_col.sort_values('index', inplace=True)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# D) Filling missing values by \n",
    "#    Overwriting the `phase_name` column by the merged filt DF's correlating column:\n",
    "# Sanity check BEFORE action -\n",
    "print(f\"Amount of missing values before action: {global_solar['phase_name'].isna().sum()}\")\n",
    "\n",
    "# Filling the mapped states into the original column by overwriting it -\n",
    "global_solar['phase_name'] = phase_col['phase_name']\n",
    "\n",
    "# Sanity check AFTER action -\n",
    "print(f\"Amount of missing values after action: {global_solar['phase_name'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cea777",
   "metadata": {},
   "source": [
    "*Result:*\n",
    "\n",
    "We've managed to fill 28 rows out of the 5350 missing values in the `phase_name` column using the non-encrypted other IDs in the `other_unit_phase_id` column. Since missing values take too much big of a share from the column, we will leave the rest of the values as they were."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e5ad7",
   "metadata": {},
   "source": [
    "### 3.4.3 Final Missing Values Check\n",
    "\n",
    "Let's check where are we standing now regarding missing values amounts, knowing that we have still left some missing values in some  columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b084e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing a guiding note for the plot -\n",
    "print('\\033[1m\\x1b[4mGuiding Note\\x1b[0m: The \\033[1m\\x1b[32mgreen\\x1b[0m horizontal line represents 100% full columns.\\n\\\n",
    "              The \\033[1m\\x1b[33morange\\x1b[0m line represents 50% full columns.\\n\\\n",
    "              The \\033[1m\\x1b[31mred\\x1b[0m line represent the 5% full threshold.')\n",
    "\n",
    "# Plotting horizontal lines representing:\n",
    "plt.axhline(0.5,0,1, c='orange', alpha=0.65, linewidth=1.5); # the full threshold\n",
    "plt.axhline(0.05,0,1, c='red', alpha=0.65, linewidth=1.5); # half-full threshold \n",
    "plt.axhline(1,0,1, c='lightgreen', alpha=0.85, linewidth=3); # less than 5% full threshold\n",
    "\n",
    "# Plotting the same missing values plot we plotted at the start of the section -\n",
    "msno.bar(global_solar, label_rotation=27, fontsize=18)\n",
    "\n",
    "# Plot modifications:\n",
    "plt.title('Share of non-missing values in each column', fontsize=32, pad=20, style='oblique');\n",
    "plt.grid()\n",
    "\n",
    "# Showing plot -\n",
    "plt.show()\n",
    "\n",
    "# Calculating the share of missing values in the DF -\n",
    "print('\\033[1m\\x1b[4mPercentage of missing values from the DF (In a descending order)\\x1b[0m:\\n\\n',\\\n",
    "      round(global_solar.isna().sum() / len(global_solar.isna()) * 100, ndigits=2)\\\n",
    "      .sort_values(ascending=False), sep='');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3099de",
   "metadata": {},
   "source": [
    "### ***3.4.4 Conclusions regarding missing values:***\n",
    "\n",
    "Out of the 7 columns we've decided worth handling, we've filled the group of columns thus:\n",
    "1. we've mapped the following location columns coordinates to finds the following addresses:**\n",
    "    - **`state_prov`** - From 6.9% to 4.23% missing values.\n",
    "        - ***252 values were filled out of 647***\n",
    "\n",
    "    - **`major_area`** - From 42.01% to 35.77% missing values.\n",
    "        - ***582 values were filled out of 3920***\n",
    "\n",
    "    - **`local_area`** - From 34.37% to 20.34% missing values.\n",
    "        - ***1309 values were filled out of 3207***\n",
    "\n",
    "    - **`city`** - From 47.39% to 30.05% missing values.\n",
    "        - ***1618 values were filled out of 4422***\n",
    "- ***The rest of the missing values in the above columns were filled with the value `'Unavailable'`.***\n",
    "\n",
    "2. We've used the numeric and non encrypted other IDs to fill `phase_name`:**\n",
    "    - **`phase_name`** - From 57.34% to 57.04% missing values.\n",
    "        - ***28 values were filled out of 5350***\n",
    "\n",
    "\n",
    "3. For missing values with local names on these columns, we've translated local names to fill values:**\n",
    "    - **`operator`** - From 35.33% to 35.26% missing values.\n",
    "        - ***7 values were filled out of 3290***\n",
    "    - **`owner`** - From 9% to 8.99% missing values.\n",
    "        - ***1 values were filled out of 840***\n",
    "- ***The rest of the missing values in the above columns were filled with the value `'Unknown'`.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626bc6e",
   "metadata": {},
   "source": [
    "## 3.5 Checking fixed data\n",
    "\n",
    "Let's have a look at the patched up DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bc7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Using info() to get a general overview of the data -\n",
    "print('\\033[1m\\x1b[4mGetting a general overview of the DF\\x1b[0m:', end='\\n\\n')\n",
    "print(global_solar.info(), end='\\n\\n')\n",
    "\n",
    "# B) Describing the columns:\n",
    "print('-----------------------------------------------------------------------\\n') # divider\n",
    "# 1. Describing the DF's columns -\n",
    "print('\\033[1m\\x1b[4mDescribing the DF\\'s columns\\x1b[0m:')\n",
    "display(global_solar.describe())\n",
    "\n",
    "# C) Sampling a random 2 rows from the DF -\n",
    "print('-----------------------------------------------------------------------\\n') # divider\n",
    "print('\\033[1m\\x1b[4mSampling 2 random rows from the DF\\x1b[0m:', end='\\n\\n')\n",
    "display(global_solar.sample(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aeb157",
   "metadata": {},
   "source": [
    "*Conclusion:*\n",
    "\n",
    "Although there are still some faults in the data, with our current data sources we've patched it the most we could."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773b7ef",
   "metadata": {},
   "source": [
    "## 3.6 Conclusions regarding Data Preprocessing\n",
    "\n",
    "In this data preprocessing chapter:\n",
    "1. **We've fixed the columns' names** to lowercased names formatted in a 'snake_case' format.\n",
    "2. **We've fixed wrong columns' data types:**\n",
    "    - The `'start_year'` and `'retired_year'` columns dtype has been changed from `float64` to `Int64`.\n",
    "    - The `'country'` and `'region'` `'status'` columns dtype has been changed from `object` to `category`.\n",
    "3. **We've validated the `'country'` column data**, sorting the data by country name alphabetically. We have also checked for duplicated countries and misspellings and found none. \n",
    "\n",
    "**4. *Handled the missing values***\n",
    "\n",
    "To decide how should we handle missing values, **we've first explored all missing values for possible connections** using some missingno missing values plots, **and we've found that overall there aren't any fully based connections between missing values**, except:\n",
    "- The `'start_year'` and `'retired_year'` columns had a full correlation of 1, which makes sense because the database author has stated that the starting year for projects who got canceled or shelved was left missing, so projects who got canceled cannot be retired.\n",
    "- Using the matrix plot, we've seen that missing values tend to look differently in different rows groups locations on the table, hinting there might be some connection between the missing values while looking at the columns for rows of similar countries. We've also seen that some missing rows of the the owner and operator columns seems to have full values on their correlating local name columns.\n",
    "\n",
    "After finding there isn't any based connection between the columns' missing values, We started dealing with the missing columns individually:\n",
    "1. I've decided to fill the missing values in the location column with their coordinates values. We've created the `'coordinates'` column, that created a string dtyped column containing the latitude and longitude in the accepted format of it. After that I've created a reversed mapping function that is used for mapping the coordinates exact addresses. We used that function to map the missing values in the following columns, while the rest of the missing values were filled with the value 'Unavailable':\n",
    "    - We mapped and filled 252 locations out of the 647 missing values in the `state_prov` column.\n",
    "    - We mapped and filled 582 locations out of the 3920 missing values in the `major_area` column.\n",
    "    - We mapped and filled 1309 locations out of the 3207 missing values in the `local_area` column.\n",
    "    - We mapped and filled 1618 exactly accurate locations out of the 1964 missing values with an `'exact'` location accuracy, and from a total amount of 4422 missing values in the `city` column itself (That's because mapping the city of approx. accurate coordinates will not be prolific).\n",
    "\n",
    "\n",
    "2. For missing values with local names on the 'local name' columns, we've translated local names to fill values, filling the remaining missing values with the value 'Unknown:\n",
    "    - **`operator`** - 7 values were filled out of 3290.\n",
    "    - **`owner`** - One value was filled out of 840.\n",
    "\n",
    "\n",
    "3. We've managed to fill 28 rows out of the 5350 missing values in the `phase_name` column using the non-encrypted other IDs in the `other_unit_phase_id` column. Since missing values take too much big of a share from the column, we left the rest of the values as they were.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922714c",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b03533",
   "metadata": {},
   "source": [
    "# 4. The Analysis\n",
    "\n",
    "Now that the data is ready, in the following section of the report, we'll visualize and analyze the data to deeper tenses, as we'll explore:\n",
    "- The energy output of solar farms. We will also map the solar farms by capacity throughout the world.\n",
    "- The total global growth in the solar industry.\n",
    "- The people who possess the biggest shares of the global solar energy market.\n",
    "\n",
    "I'll add another 2 functions that could ease the further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f025712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a display funtion that allows to display multiple DFs side by side -\n",
    "def display_sided(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h3 style=\"text-align: center;\">{title}</h2>'\n",
    "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)\n",
    "    \n",
    "# Defining a function that gets rid of the double index:\n",
    "def flat_TripleIdx(df, idx1_col, idx2_col, idx3_col=None, reset_index=True, reorder=False, dub=False):\n",
    "    '''\n",
    "    The flat_TripleIdx() function is used to flatten 3-levels MultiIndex (or Double-Index), \n",
    "    and split the tupled indices to separate columns before dropping the original index.\n",
    "    \n",
    "    Parameters:\n",
    "    df: The MultiIndexed DataFrame to flatten.\n",
    "    idx1_col: The name of the index tuple first column. Also the 1st-level in the original DataFrame's MultiIndex.\n",
    "    idx2_col: The name of the index tuple second column. Also the 2nd-level in the original DataFrame's MultiIndex.\n",
    "    idx3_col: The name of the index tuple third column. Also the 3nd-level in the original DataFrame's MultiIndex.\n",
    "    reset_index: True by default, resetting the original DataFrame's MultiIndex and dropping the old one.\n",
    "    reorder: Placing the Index column/s at the beginning of the DataFrame. False by default. \n",
    "             Currently designed only to flatten a 2-leveled MultiIndex.\n",
    "    **kwargs\n",
    "    '''\n",
    "    \n",
    "    # Flattening the `df` double (2-leveled) index -\n",
    "    df.index = df.index.to_flat_index()\n",
    "\n",
    "    # Salvaging the indices values from the tupled indices -\n",
    "    idx_1 = [x[0] for x in df.index]\n",
    "    idx_2 = [x[1] for x in df.index]\n",
    "    if dub==False:\n",
    "        idx_3 = [x[2] for x in df.index]\n",
    "\n",
    "    # re-Creating the `idx_1`, `idx_2` and `idx_3` columns -\n",
    "    df[idx1_col] = idx_1\n",
    "    df[idx2_col] = idx_2\n",
    "    if dub==False:\n",
    "        df[idx3_col] = idx_3\n",
    "        \n",
    "    # Resetting index (As long as reset_index=True)-\n",
    "    df.reset_index(drop=True,inplace=reset_index)\n",
    "    \n",
    "    # Placing the former index tuple's columns at the start of the DataFrame -\n",
    "    if reorder==True:\n",
    "        # Storing former-index column' name -\n",
    "        col_name_fir = df[idx1_col].name\n",
    "        col_name_sec = df[idx2_col].name\n",
    "        if dub==False:\n",
    "            col_name_thi = df[idx3_col].name\n",
    "        \n",
    "        # Reordering the columns -\n",
    "        # Storing columns in interior variables -\n",
    "        fir_col = df.pop(col_name_fir)\n",
    "        sec_col = df.pop(col_name_sec)\n",
    "        if dub==False:\n",
    "            thi_col = df.pop(col_name_thi)\n",
    "        # Inserting the columns at the beginning of the DataFrame -\n",
    "        if dub==False:\n",
    "            df.insert(0,col_name_thi,thi_col)\n",
    "        df.insert(0,col_name_sec,sec_col)\n",
    "        df.insert(0,col_name_fir,fir_col)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# Defining the top_n() function, which creates leaderboards for columns -\n",
    "def top_n(df, lboard_col, top5_parameter, top=5, ascend=False, xtra_lb_col=True, ranked_idx=True):\n",
    "    '''\n",
    "    The top_n() function takes a DataFrame\\'s column (i.e. leaderboard column), sort it,\n",
    "    and return the top n rows after it is sorted.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame.\n",
    "    lboard_col: The column to sort the DataFrame by; The \"leaderboard\" column.\n",
    "    top5_parameter: The values in the top-n that's returned.\n",
    "    top: Amount of leaderboard rows. Based at 5.\n",
    "    ascend: Set to False by default to return a descending leaderboard. Check True to return a rising leaderboard.\n",
    "    xtra_lb_col: Set to True by default. Return both given columns (lboard_col and top5_parameter).\n",
    "    ranked_idx: Set to True by default. Make the ranking's index start at 1.\n",
    "    '''\n",
    "    n=top\n",
    "    if xtra_lb_col== True:\n",
    "        ranking = df.sort_values(lboard_col, ascending = ascend)\\\n",
    "        .reset_index(drop=True).loc[:,[top5_parameter, lboard_col]][:n]\n",
    "    else:\n",
    "        ranking = df.sort_values(lboard_col, ascending = ascend).reset_index(drop=True)[top5_parameter][:n]\n",
    "    \n",
    "    if ranked_idx == True:\n",
    "        ranking.index += 1\n",
    "    \n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665eb1b5",
   "metadata": {},
   "source": [
    "## 4.1 Exploring Capacity\n",
    "\n",
    "To explore the capacity of solar farms, we'll need to create 2 pivot tables:\n",
    "- One summing the total and average capacity for each region, divided by countries per year.\n",
    "- Another one that does the same without years.\n",
    "\n",
    "Let's create the pivot tables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d784571",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A) Making a pivot table of the average and total capacity per region, per country, \n",
    "# divided to separate years -\n",
    "capacity_per_year = global_solar.pivot_table(values='capacity_mw', index=['start_year', 'region', 'country'], \\\n",
    "                                            dropna=False, aggfunc={'mean', 'sum'}).dropna()\n",
    "# Flattening the pivot table MultiIndex -\n",
    "flat_TripleIdx(capacity_per_year,'start_year','region','country', reorder=True)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# B) Making a pivot table of the average and total capacity per region, per country -\n",
    "capacity_per_reg = global_solar.pivot_table(values='capacity_mw', index=['region', 'country'], \\\n",
    "                                            dropna=False, aggfunc={'mean', 'sum'}).dropna()\n",
    "\n",
    "# Flattening the pivot table MultiIndex -\n",
    "flat_TripleIdx(capacity_per_reg,'region','country', dub=True, reorder=True)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# C) Displaying the pivot table -\n",
    "print('\\n\\033[1m\\x1b[4mDisplaying the DataFrames first 10 rows\\x1b[0m:')\n",
    "display_sided(capacity_per_year.head(10), capacity_per_reg.head(10),\\\n",
    "              titles=['Per Year, Divided to Regions, Countries (1st DF)', 'Per Region, Country (2st DF)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16f80d",
   "metadata": {},
   "source": [
    "### 4.1.1 Average capacity comparisons\n",
    "\n",
    "Now that the filtered pivot DFs are ready, let's plot the average capacity per region in a barplot, where each bar is divided to countries inside it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808dc906",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting a barplot showing the tota capacity per\n",
    "fig = px.bar(capacity_per_reg.sort_values('mean', ascending = False), x='region', y='mean', height=700, width=975, \\\n",
    "             color='country', color_discrete_sequence= px.colors.sequential.Rainbow_r, opacity=0.85, \\\n",
    "             title='Average Capacity per Region, Divided by Countries', barmode='stack', \\\n",
    "            hover_name='country', hover_data=['country','sum'])\n",
    "\n",
    "# Plot modification:\n",
    "fig.update_layout(title_font_size=22, font_size=14, legend_title_font_size=16, legend_title='Country', \\\n",
    "                  xaxis_title=\"Region\", yaxis_title=\"Average Capacity (MW)\", \\\n",
    "                  uniformtext_mode='hide')\n",
    "# Setting hovering spike lines -\n",
    "fig.update_xaxes(showspikes=True, spikesnap=\"cursor\", spikethickness=2)\n",
    "fig.update_yaxes(showspikes=True, spikesnap=\"cursor\", spikethickness=2)\n",
    "fig.update_layout(spikedistance=1000, hoverdistance=100)\n",
    "## Setting custom hovering data -\n",
    "fig.update_traces(hovertemplate=\"<br>\".join([\n",
    "                      \"<b>Country:</b> %{customdata[0]}\",\n",
    "                      '',\n",
    "                      \"Average Capacity (MW): %{y}\",\n",
    "                      \"All-farms Total Capacity (MW): %{customdata[1]}\",\n",
    "                      \"Region: %{x}\"]))\n",
    "# Show plot -\n",
    "fig.show()\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# Creating some rankings -\n",
    "highest_outpout_countries = (top_n(capacity_per_reg, 'mean', 'country', top=10))\n",
    "lowest_outpout_countries = (top_n(capacity_per_reg, 'mean', 'country', top=10, ascend=True))\n",
    "# Changing rankings columns' names for display -\n",
    "highest_outpout_countries.columns = ['Country', 'Average MWs per Project']\n",
    "lowest_outpout_countries.columns = ['Country', 'Average MWs per Project']\n",
    "\n",
    "# Printing rankings -\n",
    "print('\\n\\033[1m\\x1b[4mThe top 10 countries with highest and lowest average energy output\\x1b[0m:', end='')\n",
    "display_sided(round(highest_outpout_countries, ndigits=2), round(lowest_outpout_countries, ndigits=2), \\\n",
    "              titles=['Highest Output', 'Lowest Output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef2320",
   "metadata": {},
   "source": [
    "#### ***Analyzing the regions***\n",
    "\n",
    "As we can observe, the region with the highest average energy output is Asia, with an average of 5405 MWs per solar farm project, having more than doubled average energy output than each of the Europe and the other lowest energy output regions. Followed by Asia are the Middle East region (averagely 4370 Mws per a solar farm project), Africa (averagely 3752 MWs per a solar farm project), and Europe (averagely 2400 MWs per a solar farm project); These top 4 regions have a much higher average energy output than the other five regions, As all of the top regions have at least more than 3.5 times of any of these last regions. The region with the lowest average energy output is North America, with 252 MWs per each solar project.\n",
    "\n",
    "If we ignore North Korea, which has only one solar facility and is bumping the average energy output for Asia drastically (Try clicking the country on the interactive plot's legend), we can see that The region that produces the highest energy output is actually the Middle East region, and that Asia is dropping to the third place below Africa without North Korea.\n",
    "\n",
    "#### ***Analyzing the countries***\n",
    "\n",
    "Zooming to the micro-level, the leading countries are Oman (averagely 1439.09 MWs per average solar project, Middle East), Kuwait (averagely 1265 MWs per solar project, Middle East), and Morocco (averagely 539.33 MWs per solar project, Africa). North Korea (Asia) average 2500 MWs, but that's just because it has only one solar farm overall. The countries with the absolute lowest energy output are Somalia and Sudan, tied at the first place with an average of 10 MWs per solar project, which is also minimum capacity allowed in Arab countries. Following them are a list of countries - as both Barbados, South Sudan, Madagascar, Liberia and Guinea-Bissau average a capacity of 20 MWs per solar project in the heavily tied second place.\n",
    "\n",
    "Seems like the leading countries are tending to be usually Arab countries and the countries with the lowest average energy output are tending to be usually from third-world countries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e33ce",
   "metadata": {},
   "source": [
    "### 4.1.2 Mapping global energy output\n",
    "\n",
    "We'll use the coordinates in the `global_solar` DF to map all solar farm projects on the world map so that we could see the global energy output of all solar farms visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5de98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolating circle radius to given circle range -\n",
    "rad = global_solar.capacity_mw.values.tolist()\n",
    "m = intp.interp1d([1,max(rad)], [5,77.5])\n",
    "circ_rads = m(rad)\n",
    "\n",
    "# Plotting the energy\n",
    "fig = px.density_mapbox(global_solar, lat='latitude', lon='longitude', zoom=0.75, radius=circ_rads, \\\n",
    "                        height=675, width=975, \\\n",
    "                        color_continuous_scale=px.colors.sequential.Hot, mapbox_style='carto-darkmatter',\\\n",
    "                        hover_data=['country','capacity_mw','region','city', 'local_area','major_area',\\\n",
    "                                    'project_name','operator','owner'])\n",
    "\n",
    "# Plot modifications:\n",
    "fig.update_layout(title='Global Energy Output', title_font_size=32,legend_title_font_size=22)              \n",
    "# Setting custom hovering data -\n",
    "fig.update_traces(hoverlabel_bgcolor='darkred', hoverlabel_bordercolor='gold',\n",
    "    hovertemplate=\"<br>\".join([\n",
    "        \"<b>Project:</b> %{customdata[6]}\",\n",
    "        'Operations are ran by %{customdata[7]}',\n",
    "        'Owned by %{customdata[8]}',\n",
    "        '',\n",
    "        \"Coordinates: %{lat},%{lon}\",\n",
    "        \"Capacity (MW): %{customdata[1]}\",\n",
    "        \"Country: %{customdata[0]}\",\n",
    "        \"City: %{customdata[3]}\",\n",
    "        \"Area (County|District): %{customdata[4]} | %{customdata[5]}\",\n",
    "        \"Region: %{customdata[2]}\"\n",
    "    ]))\n",
    "\n",
    "# Showing the density maplot -\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98d470",
   "metadata": {},
   "source": [
    "We can see pretty strongly that Asia has the highest Energy output overall, and that solar farms are distributed pretty equally around east to central Asia, and in India as well. Europe is also pretty distributed with heavy energy output farms around mostly Italy, The UK, and western Europe (i.e. Spain and Portugal). On the other hand, solar farms on the Middle east aren't distributed so heavily and it looks like there are also less farms than the other regions we spoke about before, while this might be explained by that in this region each solar plant produces a bigger energy output than the regular solar farms around Asia and Europe.\n",
    "\n",
    "***The leading energy output farms***\n",
    "\n",
    "You could also notice a big red blur in North Australia that's located in the city of Elliot on Barkley Region county. It represents the Powell Creek solar farm which is the most productive solar farm **with an energy output of 20K MegaWatts**. After that farm, the leading farms globally are the Al Wusta Solar Plant in Oman (12.5k MWs), which is located in the wastelands at the south-west part of the country, and the Ladakh Solar park in India (10k MWs), which is located in the city of Leh in the Nubra county.\n",
    "\n",
    "***The north is empty***\n",
    "\n",
    "We can see that there are *barely* any solar farms in the north, to be specific - in Russia, the Norse countries (except Denmark), Greenland, Iceland and North Canada. That might be because these regions don't have enough sun light, light hours or heat to be prolific enough for solar energy production. On the furthest contrary side, **Finland is an exception as it has the northernmost 2 solar farms in the entire world**, the Lapua and Palloneva solar farms, which are producing 80 MWs and 500 MWs, orderly. **Lapua solar farm is the northernmost farm of all**, located in the city of Lapua at the Seinäjoki sub-region.\n",
    "\n",
    "***The higher the sun shine***\n",
    "\n",
    "There is also another interesting fact that requires attention, **the majority of the solar farms are located on the coast, or in countries that are close to the coast** - this can be seen perfectly in Africa, where the solar farms who produce the highest energy signatures are the ones close to the coast. Also note that each of East Asia, East Australia, India and both the Americas shows the same supporting evidence to that assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b35523",
   "metadata": {},
   "source": [
    "## 4.2 Exploring amounts of solar farms throughout the years\n",
    "\n",
    "In order to explore the change in the amounts of solar farms throughout the years, we'll create a pivot table that shows the amount of projects in every country, region throughout the years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pivot table showing the amount of projects started in each recorded year -\n",
    "proj_p_y = global_solar.pivot_table(values='project_name', \\\n",
    "                                            index=['start_year', 'region', 'country'], aggfunc='count')\n",
    "\n",
    "# Flatening MultiIndex -\n",
    "flat_TripleIdx(proj_p_y, 'start_year', 'region', 'country', reorder=True)\n",
    "\n",
    "# Displaying the pivot table -\n",
    "proj_p_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4b58a",
   "metadata": {},
   "source": [
    "The data we've created represents each country's solar farms amount per a recorded year. \n",
    "\n",
    "Now that the data is ready, let's plot the growth rates in solar farms for each region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da679a63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the cumulative amounts of solar farms in each region -\n",
    "fig = px.ecdf(proj_p_y, x='start_year', y='project_name', color='region', \\\n",
    "             color_discrete_sequence= px.colors.sequential.Plasma_r, height=745, width=1000, \\\n",
    "             range_x=(2006.5,2028), ecdfnorm=None, marginal='box', markers=True, \\\n",
    "             hover_data=['region', 'start_year', 'project_name'])\n",
    "\n",
    "# Plot modifications:\n",
    "fig.update_layout(title='Total number of solar farms per Region', title_font_size=22,\\\n",
    "                  legend_title='Region:', legend_title_font_size=22, legend_font_size=12,\n",
    "                  xaxis_title=\"Year\", yaxis_title=\"Amount of Solar Farms\", \\\n",
    "                  font_size=16, uniformtext_mode='hide')\n",
    "# Setting hovering spike lines -\n",
    "fig.update_xaxes(showspikes=True, spikethickness=2, spikesnap=\"cursor\", spikemode=\"across\")\n",
    "fig.update_yaxes(showspikes=True, spikethickness=2, spikemode=\"across\")\n",
    "fig.update_layout(spikedistance=500, hoverdistance=25)\n",
    "# Setting custom hovering data -\n",
    "fig.update_traces(hoverlabel_bgcolor='gold', hoverlabel_bordercolor='rebeccapurple',\n",
    "                  hovertemplate=\"<br>\".join([\n",
    "                      \"<b>Region:</b> %{customdata[0]}\",\n",
    "                      '',\n",
    "                      \"Amount of farms: %{y}\",\n",
    "                      \"Year: %{x}\"]))\n",
    "# Adding delimiting line & area to the plot\n",
    "# to mark future forecasts according to data on future projects -\n",
    "fig.add_vline(x=2023, line_width=2, line_dash=\"dash\", line_color=\"rebeccapurple\")\n",
    "fig.add_vrect(x0=2023, x1=2030, line_width=0, fillcolor=\"rebeccapurple\", opacity=0.15, \\\n",
    "             annotation_text=\"Forecasts\", annotation_position=\"top left\", \\\n",
    "             annotation=dict(font_size=18),)\n",
    "\n",
    "# Showing plot -\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c5bc8",
   "metadata": {},
   "source": [
    "*Plot Note: The data on the faded purple frame are on the right represents known future projects that are expected to start operating in the following years.*\n",
    "\n",
    "Looking at the marginal boxplots above the graph, we can see that all of the countries are equally distributed, while the peak of globally growth has been reached to in the year of 2019 - since that year, growth rate seems to have been faded in all regions equally. That global decline in new solar numbers could possibly be explained by the global Coronavirus shutdown in 2020 to the beginning of 2021, that might have shelved, canceled or delayed a lot of projects from starting to run.\n",
    "\n",
    "We can clearly see that Asia has the highest number of solar farms, by a supreme distance over all others regions - as since 2013, a huge growth rate trendline has started for the region, growing by at least 300 solar farms year to year up until the 2019 peak. The runner up is surprisingly North America, which might be really low on total energy output, but are opening more a more solar plants throughout the years. Central America and the Caribbean is the region that had the weakest growth rate with the lowest number of solar projects 42 in the current year (2022) while the following regions are the Eurasia with 42 solar farms, and the Middle East with 64 farms in 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e0025",
   "metadata": {},
   "source": [
    "## 4.3 Top ownership companies and individuals\n",
    "\n",
    "We'll check which companies own the most solar farms globally to determine has the biggest control over the solar market.\n",
    "\n",
    "Let's begin by creating a pivot table for companies who own the most solar farm projects, and then create another one for finding the top 10 owners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6192b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pivot table that shows the companies who own the most solar farms globally -\n",
    "owner_ranks = global_solar.pivot_table(values='project_name',index='owner', aggfunc='count')\n",
    "\n",
    "# Flattening the index and sorting the values as descending -\n",
    "owner_ranks = flat_Idx(owner_ranks,'owner', reorder=True).sort_values(by='project_name', ascending=False)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# Finding the top 10 owners who are not unknown -\n",
    "top10_owners = top_n(owner_ranks.query('owner != \"Unknown\"'),'project_name','owner',10)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# Displaying DFs -\n",
    "print('\\n\\033[1m\\x1b[4mDisplaying the \"Owners Ranking\" and the top 10 ranked owners (The leading 10 owners)\\x1b[0m:', end='')\n",
    "display_sided(owner_ranks.head(10), top10_owners.head(10), \\\n",
    "             titles=['The `owner_ranks` DF', 'The `top10_owner` DF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1bf42",
   "metadata": {},
   "source": [
    "Now that the DFs are created. we'll plot a barplot representing the globally leading 10 ownerships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the leading 10 owners in the industry -\n",
    "fig = px.bar(top10_owners,x='owner',y='project_name', color='owner', \\\n",
    "      title='Companies who own the most solar farms', height=600, width=950,\\\n",
    "            color_discrete_sequence= px.colors.sequential.solar_r)\n",
    "\n",
    "# Plot modifications:\n",
    "fig.update_layout(title='Total solar farms for the Top 10 Ownerships', title_font_size=22,\\\n",
    "                  legend_title='Ownership:', legend_title_font_size=16, legend_font_size=12,\n",
    "                  xaxis_title=\"Ownership Company\", yaxis_title=\"Amount\", \\\n",
    "                  font_size=14, uniformtext_mode='hide')\n",
    "fig.update_xaxes(tickangle = 25)\n",
    "# Setting hovering spike lines -\n",
    "fig.update_yaxes(showspikes=True, spikethickness=2, spikemode=\"across\")\n",
    "fig.update_layout(spikedistance=500, hoverdistance=25)\n",
    "# Setting custom hovering data -\n",
    "fig.update_traces(hoverlabel_bgcolor='gold', hoverlabel_bordercolor='rebeccapurple',\n",
    "                  hovertemplate=\"<br>\".join([\n",
    "                      \"<b>Owner:</b> %{x}\",\n",
    "                      \"<b>Owned solar farms:</b> %{y}\"]))       \n",
    "# Showing plot -\n",
    "fig.show()\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# Calculating some stats -\n",
    "unique_owners = len(owner_ranks['owner'])\n",
    "more_than_10 = len(owner_ranks[(owner_ranks['owner'] != 'Unknown') & (owner_ranks['project_name'] > 10)])\n",
    "less_than_10 = len(owner_ranks[(owner_ranks['owner'] != 'Unknown') & (owner_ranks['project_name'] < 10)])\n",
    "\n",
    "# Printing stats -\n",
    "print(f\"\\033[1m\\x1b[4mSome stats\\x1b[0m:\\n\\n\\\n",
    "There are \\033[1m\\x1b[4m{unique_owners} ownership companies\\x1b[0m around the solar market.\\n\\\n",
    "Companies who have more than 10 farms: \\033[4m\\x1b[31m{more_than_10} ownership companies\\x1b[0m.\\n\\\n",
    "Companies who have less than 10 farms: {less_than_10} ownership companies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa2034e",
   "metadata": {},
   "source": [
    "The company who owns the most solar farms around the world is NextEra Energy, which has 140 solar farms under it's command, more than 2 times of the other 3rd to 10th places in the ranking, and about 1.7 times the 2nd place amount; NextEra Energy are taking a much bigger part of the solar energy solution to the oil and petrol recession than the entire industry, pretty much controlling the Solar Energy market.\n",
    "\n",
    "The following leading ownerships companies were the CECEP Solar Energy Technology company, having 82 solar farms under it's belt, the CGN Solar Energy Development company, that possess 70 solar farms globally, and Solatio Energia who owns 62 solar farms. The rest of the leading 10 have pretty similar amounts that range from Adani Green Energy's 56 solar farms to Duke Energy's 48 farms.\n",
    "\n",
    "Be noted that right now, out of the 3453 known ownership companies or individuals, Only 112 owners possess more than 10 solar farms overall, i.e. there are 3323 ownerships that are under-represented in the solar market, while the leading companies control the majority. There's a cartel union in the solar farms market, where the top companies control a much bigger bigger share than the rest of the small ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33aa62",
   "metadata": {},
   "source": [
    "------------------\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb855491",
   "metadata": {},
   "source": [
    "# 5. General Conclusion\n",
    "\n",
    "At the first time we observed the data during the Data Exploration section, we've noticed some problems:\n",
    "- All columns' names were capitalized, while some possessed really long names.\n",
    "- There were 15 columns with missing values, out of which 8 columns are clearly fine to ignore. The columns who required the special attention were:\n",
    "    - **`Phase Name`**\n",
    "    - **`City`**\n",
    "    - **`Major area (prefecture, district)`**\n",
    "    - **`Operator`**\n",
    "    - **`Local area (taluk, county)`**\n",
    "    - **`Owner`**\n",
    "    - **`State/Province`**\n",
    "- In the 'About' section of the database, the author of the data advised to check the consistency across the `'Country` column trackers, although stating it should have been ordered.\n",
    "\n",
    "## **5.1 Data Preprocessing**\n",
    "\n",
    "We started preprocessing the data in the following ways:\n",
    "\n",
    "***1. Fixing columns names:***\n",
    "\n",
    "We started by changing all the columns' names to short and `'snake_cased'` names, so they'll be easier to work with.\n",
    "\n",
    "***2. Wrong dtypes***\n",
    "\n",
    "We've fixed wrong columns' data types:\n",
    "    - The `'start_year'` and `'retired_year'` columns dtype has been changed from `float64` to `Int64`.\n",
    "    - The `'country'` and `'region'` `'status'` columns dtype has been changed from `object` to `category`.\n",
    "\n",
    "***3. Validating data on the `'country'` column***\n",
    "\n",
    "We've noticed that the `'country'` column was sorted alphabetically only partly on the first rows, but on the plus side there are no misspelling or partial duplicated country names. To fix this we have sorted the data by the `'country'` column and resetted the index.\n",
    "\n",
    "***4. Handling the missing values***\n",
    "\n",
    "To decide how should we handle missing values, we've first explored all missing values for possible connections using some 'missingno' missing values plots, and we've found that overall there aren't any fully based connections between missing values, except:\n",
    "- The `'start_year'` and `'retired_year'` columns had a full correlation of 1, which makes sense because the database author has stated that the starting year for projects who got canceled or shelved was left missing, so projects who got canceled cannot be retired.\n",
    "- Using the matrix plot, we've seen that missing values tend to look differently in different rows groups locations on the table, hinting there might be some connection between the missing values while looking at the columns for rows of similar countries. We've also seen that some missing rows of the the owner and operator columns seems to have full values on their correlating local name columns.\n",
    "\n",
    "***4.1 Afterwards, we've started handling the 7 columns that we've decided that should be handled in prior stages:***4.1\n",
    "1. I've decided to fill the missing values in the location column with their coordinates values. We've created the `'coordinates'` column, that created a string dtyped column containing the latitude and longitude in the accepted format of it. After that I've created a reversed mapping function that is used for mapping the coordinates exact addresses. We used that function to map the missing values in the following columns, while the rest of the missing values were filled with the value 'Unavailable':\n",
    "    - We mapped and filled 252 locations out of the 647 missing values in the `state_prov` column.\n",
    "    - We mapped and filled 582 locations out of the 3920 missing values in the `major_area` column.\n",
    "    - We mapped and filled 1309 locations out of the 3207 missing values in the `local_area` column.\n",
    "    - We mapped and filled 1618 exactly accurate locations out of the 1964 missing values with an `'exact'` location accuracy, and from a total amount of 4422 missing values in the `city` column itself (That's because mapping the city of approx.. accurate coordinates will not be prolific).\n",
    "\n",
    "\n",
    "2. For missing values with local names on the 'local name' columns, we've translated local names to fill values, filling the remaining missing values with the value 'Unknown:\n",
    "    - **`operator`** - 7 values were filled out of 3290.\n",
    "    - **`owner`** - One value was filled out of 840.\n",
    "\n",
    "\n",
    "3. We've managed to fill 28 rows out of the 5350 missing values in the `phase_name` column using the non-encrypted other IDs in the `other_unit_phase_id` column. Since missing values take too much big of a share from the column, we left the rest of the values as they were.\n",
    "\n",
    "## **5.2 The Analysis and Data Visualization**\n",
    "\n",
    "We've explored a few areas in that section, as my top conclusions were as follows.\n",
    "\n",
    "### ***Conclusions regarding capacity output:***\n",
    "\n",
    "Asia is the region that produces the highest energy output in the entire world, but without North Korea, it's dropping to the 3rd place, symbolizing that it constitutes a huge part of Asia leading the solar market output. The Middle East region produces a lot of energy, and **although it has less solar farms, each farm is producing much more solar energy averagely than in any other region**. Solar farm located in Europe have the exact opposite story, as they tend to have a smaller capacity than most solar farms in the big regions, but are compensating mostly in the numbers, as they have a lot of solar farms distributed mostly in Greece, The UK, Italy and West Europe (a.k.a Spain and Portugal) shores, majorly. solar farms located in Africa have a pretty high average capacity and great sunlight conditions, making their relatively small amount of solar farms powerful enough to compete with the big regions, although the vast majority of the solar farms in the region are located near the edges of the continent, while most of central Africa seems to lack solar farms.\n",
    "\n",
    "There seems to be a big effect on climate and location on the successful output of a solar farm, as the countries who produce the most solar energy are usually Arab countries, that are located in hot climate areas like Africa or close to the coast, where they get a lot more sunlight than shore-less countries. A special notation should go to Australia, who has a huge amount of low capacity solar plants in the east of the continent, but are compensating it with the Power Creek solar farm, which is the largest solar energy producer on the globe.\n",
    "\n",
    "There is a black hole lacking energy output in the north side of the globe, that seems to result from the lack of the conditions discussed in the above paragraph - having a cold climate, low amount of sunlight and smaller daytime. On the other hand, the country of Finland specifically, which is known for having a surprisingly great climate *although* it's geographical location, is the northernmost place in the world for a solar farm to appear.\n",
    "\n",
    "#### **To create an exact profile:**\n",
    "- The countries with ***the highest*** average solar energy production are tending to be usually Arab countries, with great access to heat and sunlight, located near the coast or in a country close to the coast.\n",
    "- The countries with ***the lowest*** average energy production are tending to be usually poor third-world countries, located far from shores, or in the north side of the globe.\n",
    "\n",
    "### ***Conclusions regarding the growth in the amounts of solar energy production:***\n",
    "\n",
    "It seems like even though the solar farms market has started pretty low, it has gained popularity in the beginning of the last decade, around 2012-2013, while Asian countries have embraced the trend much faster than countries from all other regions, even though we could clearly see a huge drop in numbers of new solar farms since 2020, which could easily be explained by the Coronavirus pandemic global shutdown which has affected the entire world, that was probably the reason it was hard for business owners to open new farms.\n",
    "\n",
    "Asia has the highest growth rate around all regions, having at least 300 new solar facilities built over every year, up until the global shutdown in 2020, Asia tallied a total of 3461 solar farms in 2022. Meanwhile, we can see that although North America is the region that produces the lowest solar energy around the world, it actually ignored the shutdown in 2020 and has started to rapidly grow in amounts. The region that has the lowest amount of solar farms in 2022 is Central America and The Caribbeans with 42 farms, and it also has the lowest growth rate. The Middle East region currently has 64 solar plants in 2022.\n",
    "\n",
    "### ***Conclusions regarding ownerships in the solar energy market:***\n",
    "\n",
    "There seems to be a carte union in the global solar energy market, as while there are 3453 recorded ownerships of solar farms globally, only 112 companies out of them own more than 10 farms, and the top 4 owners each hold more than 62 farms each. The company that controls the biggest share of the solar market is the NextEra Energy company with an owned quantity of 140 solar farms.\n",
    "\n",
    "## 5.3 Final words\n",
    "\n",
    "While there might have been a slowdown in the past 2 years because of the global pandemic, the future is looking bright and sunny, as more and more countries are starting to invest in solar energy as an alternate main energy resource in preparation for the eradication petrol, while the leaders of the revolution are Asian, Middle Eastern and African countries and the pioneering NextEra company, along with the rest of the solar farms investors. As when the growth rate will return to it's normal self, in a few years we might be using the sun as our main energy source and make the Earth greener."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
